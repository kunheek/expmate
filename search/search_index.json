{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"ExpMate","text":"<p>ML Research Boilerplate \u2014 Config &amp; Logging First</p> <p>Welcome to ExpMate, a lightweight experiment management toolkit designed for ML researchers who want to focus on their experiments, not on boilerplate code.</p>"},{"location":"#overview","title":"Overview","text":"<p>ExpMate provides clean, reusable patterns for configuration management, logging, and experiment tracking\u2014everything you need to run reproducible ML experiments.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>\ud83d\udd27 Configuration Management: YAML-based configs with command-line overrides</li> <li>\ud83d\udcca Experiment Logging: Structured logging with metrics tracking</li> <li>\ud83d\ude80 PyTorch Integration: Checkpoint management and DDP utilities</li> <li>\ud83d\udcc8 Experiment Tracking: Built-in support for WandB and TensorBoard</li> <li>\ud83d\udd0d CLI Tools: Compare runs, visualize metrics, and manage sweeps</li> <li>\ud83d\udd04 Git Integration: Automatic git info tracking for reproducibility</li> </ul>"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code>from expmate import ExperimentLogger, parse_config, set_seed\n\n# Parse config from YAML + command-line overrides\nconfig = parse_config()\n\n# Set random seed for reproducibility\nset_seed(config.seed)\n\n# Create experiment logger\nlogger = ExperimentLogger(run_dir=f\"runs/{config.run_id}\")\nlogger.info(f\"Starting experiment: {config.run_id}\")\n\n# Your training code here...\nfor epoch in range(config.training.epochs):\n    # ... training logic ...\n\n    # Log metrics\n    logger.log_metric(step=epoch, split='train', name='loss', value=loss)\n    logger.info(f\"Epoch {epoch}: loss={loss:.4f}\")\n</code></pre>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install expmate\n</code></pre> <p>For optional tracking platforms:</p> <pre><code># Weights &amp; Biases\npip install expmate[wandb]\n\n# TensorBoard\npip install expmate[tensorboard]\n\n# Both tracking platforms\npip install expmate[tracking]\n\n# Development tools\npip install expmate[dev]\n</code></pre> <p>Note: PyTorch is not bundled. Install it separately based on your system requirements.</p>"},{"location":"#next-steps","title":"Next Steps","text":"<ul> <li>Installation Guide</li> <li>Quick Start Tutorial</li> <li>Basic Concepts</li> <li>API Reference</li> </ul>"},{"location":"#support","title":"Support","text":"<ul> <li>GitHub: kunheek/expmate</li> <li>Issues: Issue Tracker</li> <li>Email: kunhee.kim@kaist.ac.kr</li> </ul>"},{"location":"ARGUMENT_PARSER/","title":"Config Override Pattern with LSP Support","text":"<p>The recommended pattern for using argument parsing with Config provides full LSP support for type inference, autocomplete, and go-to-definition.</p>"},{"location":"ARGUMENT_PARSER/#why-this-pattern","title":"Why This Pattern?","text":"<p>\u2705 Full LSP Support - Type inference works perfectly \u2705 Simple - Uses standard argparse, no custom wrappers \u2705 Clear - Explicit separation of CLI args and config \u2705 Flexible - Full control over both argparse and config  </p>"},{"location":"ARGUMENT_PARSER/#recommended-pattern","title":"Recommended Pattern","text":"<pre><code>import argparse\nfrom dataclasses import dataclass\nfrom expmate import Config, override_config\n\n@dataclass\nclass TrainingConfig(Config):\n    lr: float = 0.001\n    batch_size: int = 32\n    epochs: int = 100\n\n# Standard argparse\nparser = argparse.ArgumentParser()\nparser.add_argument(\"--config-file\", type=str, default=None)\nparser.add_argument(\"--seed\", type=int, default=42)\nparser.add_argument(\"--device\", type=str, default=\"cuda\")\n\n# Parse arguments\nargs, unknown = parser.parse_known_args()\n\n# Load config - LSP knows exact type!\nif args.config_file:\n    config = TrainingConfig.from_file(args.config_file)  # \u2728 LSP: TrainingConfig\nelse:\n    config = TrainingConfig.from_dict({})  # \u2728 LSP: TrainingConfig\n\n# Apply + prefix overrides - Type is preserved!\nconfig = override_config(config, unknown)  # \u2728 LSP still knows: TrainingConfig\n\n# Use with full autocomplete!\nprint(config.lr)  # \u2728 LSP autocompletes .lr, .batch_size, .epochs\n</code></pre>"},{"location":"ARGUMENT_PARSER/#lsp-benefits","title":"LSP Benefits","text":"<p>The type information is preserved throughout the entire workflow:</p> <ol> <li>From file/dict - <code>config = TrainingConfig.from_file(...)</code> \u2192 LSP knows it's <code>TrainingConfig</code></li> <li>After overrides - <code>config = override_config(config, unknown)</code> \u2192 LSP still knows it's <code>TrainingConfig</code>!</li> <li>Autocomplete - IDE suggests <code>config.lr</code>, <code>config.batch_size</code>, <code>config.epochs</code></li> <li>Go-to-Definition - Jump directly to the dataclass field</li> <li>Type Checking - Static type checkers know the exact type</li> <li>Refactoring - Rename fields across entire codebase</li> </ol>"},{"location":"ARGUMENT_PARSER/#command-line-usage","title":"Command Line Usage","text":"<pre><code># Use defaults\npython train.py --seed 123\n\n# Load config from file\npython train.py --config-file config.yaml --seed 123\n\n# Override config with + prefix\npython train.py --seed 123 +lr=0.01 +batch_size=64\n\n# Combine all\npython train.py --config-file config.yaml --seed 123 +lr=0.01 +epochs=200\n\n# Add new dynamic config\npython train.py +experiment.name=\"test\" +debug=true\n\n# Pass sequences/lists naturally\npython train.py +layers 64 128 256 512\npython train.py +ids:int 1 2 3 4 5\npython train.py +learning_rates:float 0.1 0.01 0.001\n\n# Mix sequences with other overrides\npython train.py --config-file config.yaml +lr=0.01 +layers 128 256 512 +epochs=200\n</code></pre>"},{"location":"ARGUMENT_PARSER/#config-override-formats","title":"Config Override Formats","text":"<p>The <code>override_config()</code> function supports these formats with <code>+</code> prefix:</p> Format Example Description <code>+key=value</code> <code>+lr=0.01</code> Basic key-value <code>+key value</code> <code>+lr 0.01</code> Space-separated <code>+key val1 val2 val3</code> <code>+layers 64 128 256</code> Sequence/list (auto-detect types) <code>+nested.key=value</code> <code>+model.hidden_dim=512</code> Nested config <code>+key:type=value</code> <code>+lr:float=0.01</code> With type hint <code>+key:type value</code> <code>+lr:float 0.01</code> Type hint, space-separated <code>+key:type v1 v2 v3</code> <code>+ids:int 1 2 3</code> Typed sequence/list"},{"location":"ARGUMENT_PARSER/#warning-system","title":"Warning System","text":"<p>If you accidentally pass <code>-</code> or <code>--</code> prefixed args to <code>override_config()</code>, you'll get a warning:</p> <pre><code>args, unknown = parser.parse_known_args([\"--ignored\", \"+lr=0.01\"])\nconfig = override_config(config, unknown)\n# \u26a0\ufe0f  Warning: Found arguments with '-' or '--' prefix: ['--ignored'].\n#     These will be ignored by override_config().\n#     Use '+' prefix for config overrides (e.g., +key=value).\n</code></pre>"},{"location":"ARGUMENT_PARSER/#complete-example","title":"Complete Example","text":"<pre><code>import argparse\nfrom dataclasses import dataclass\nfrom expmate import Config, override_config\n\n@dataclass\nclass TrainingConfig(Config):\n    \"\"\"Training configuration with full LSP support.\"\"\"\n    lr: float = 0.001\n    batch_size: int = 32\n    epochs: int = 100\n    optimizer: str = \"adam\"\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Training script\")\n    parser.add_argument(\"--config-file\", type=str, default=None)\n    parser.add_argument(\"--seed\", type=int, default=42)\n    parser.add_argument(\"--device\", type=str, default=\"cuda\")\n    parser.add_argument(\"--verbose\", action=\"store_true\")\n\n    args, unknown = parser.parse_known_args()\n\n    # Load config with LSP support\n    if args.config_file:\n        config = TrainingConfig.from_file(args.config_file)\n    else:\n        config = TrainingConfig.from_dict({})\n\n    # Override with + prefix args\n    config = override_config(config, unknown)\n\n    # Use with full IDE support!\n    print(f\"Training with lr={config.lr}, batch_size={config.batch_size}\")\n    print(f\"Seed: {args.seed}, Device: {args.device}\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>Run with: <pre><code>python train.py --config-file config.yaml --seed 999 +lr=0.005 +batch_size=128\n</code></pre></p>"},{"location":"ARGUMENT_PARSER/#why-not-a-custom-parser","title":"Why Not a Custom Parser?","text":"<p>We previously had <code>ArgumentParser</code> and <code>ConfigArgumentParser</code> wrapper classes, but removed them because:</p> <ol> <li>LSP doesn't work - Return type <code>config</code> is not explicitly typed</li> <li>Too complex - Custom wrappers add unnecessary complexity  </li> <li>Less flexible - Can't use full argparse features</li> <li>Standard is better - Everyone knows argparse</li> </ol> <p>The recommended pattern is simpler, more explicit, and gives perfect LSP support! \u2728</p>"},{"location":"ARGUMENT_PARSER/#nested-configs","title":"Nested Configs","text":"<p>You can define config classes with nested structure for better organization:</p> <pre><code>from dataclasses import dataclass, field\nfrom expmate import Config, override_config\n\n@dataclass\nclass ModelConfig(Config):\n    \"\"\"Model architecture configuration.\"\"\"\n    dim: int = 128\n    layers: int = 4\n    dropout: float = 0.1\n\n@dataclass\nclass OptimizerConfig(Config):\n    \"\"\"Optimizer configuration.\"\"\"\n    name: str = \"adam\"\n    lr: float = 0.001\n    betas: list[float] = field(default_factory=lambda: [0.9, 0.999])\n\n@dataclass\nclass TrainingConfig(Config):\n    \"\"\"Main config with nested configs.\"\"\"\n    seed: int = 42\n    epochs: int = 100\n\n    # Nested configs - use field(default_factory=...)\n    model: ModelConfig = field(default_factory=ModelConfig)\n    optimizer: OptimizerConfig = field(default_factory=OptimizerConfig)\n\n# Load and use\nconfig = TrainingConfig.from_dict({})\nconfig = override_config(config, unknown)\n\n# LSP autocompletes everything!\nprint(config.model.dim)  # \u2728 Autocomplete: .dim, .layers, .dropout\nprint(config.optimizer.lr)  # \u2728 Autocomplete: .name, .lr, .betas\n</code></pre> <p>Override nested configs from command line:</p> <pre><code># Override nested fields with dot notation\npython train.py +model.dim=512 +model.layers=12\npython train.py +optimizer.lr=0.0001 +optimizer.name=adamw\n\n# Override sequences in nested configs\npython train.py +optimizer.betas:float 0.95 0.999\n\n# Mix top-level and nested overrides\npython train.py +seed=999 +model.dim=256 +optimizer.lr=0.01\n</code></pre> <p>See <code>examples/example_nested_config.py</code> for a complete example.</p>"},{"location":"LSP_FIX_SUMMARY/","title":"LSP Type Inference Fix for override_config()","text":""},{"location":"LSP_FIX_SUMMARY/#problem","title":"Problem","text":"<p>Previously, after calling <code>override_config()</code>, LSP autocomplete stopped working because the function signature was:</p> <pre><code>def override_config(config: \"Config\", unknown_args: List[str]) -&gt; \"Config\":\n    ...\n</code></pre> <p>This meant that even if you passed in a <code>TrainingConfig</code>, the return type was just <code>Config</code>, causing LSP to lose the specific type information.</p>"},{"location":"LSP_FIX_SUMMARY/#solution","title":"Solution","text":"<p>Changed the function signature to use a generic TypeVar:</p> <pre><code>OverrideConfigT = TypeVar(\"OverrideConfigT\", bound=\"Config\")\n\ndef override_config(\n    config: OverrideConfigT, unknown_args: List[str]\n) -&gt; OverrideConfigT:\n    ...\n</code></pre> <p>Now the return type matches the input type!</p>"},{"location":"LSP_FIX_SUMMARY/#result","title":"Result","text":"<p>\u2705 Before the fix: <pre><code>config = TrainingConfig.from_dict({})  # LSP knows: TrainingConfig\nconfig = override_config(config, unknown)  # LSP thinks: Config \u274c\n# Autocomplete broken! config.&lt;no suggestions&gt;\n</code></pre></p> <p>\u2705 After the fix: <pre><code>config = TrainingConfig.from_dict({})  # LSP knows: TrainingConfig\nconfig = override_config(config, unknown)  # LSP knows: TrainingConfig \u2705\n# Autocomplete works! config.&lt;lr, batch_size, epochs, model&gt;\n</code></pre></p>"},{"location":"LSP_FIX_SUMMARY/#testing","title":"Testing","text":"<p>Run <code>test_lsp_inference.py</code> to verify:</p> <pre><code>python3 test_lsp_inference.py +lr=0.01 +model.dim=512\n</code></pre> <p>Then open the file in VS Code and verify autocomplete works on the line after <code>config = override_config(config, unknown)</code>.</p>"},{"location":"LSP_FIX_SUMMARY/#files-changed","title":"Files Changed","text":"<ul> <li><code>src/expmate/config.py</code>: Added <code>OverrideConfigT</code> TypeVar and updated <code>override_config()</code> signature</li> <li><code>test_lsp_inference.py</code>: Test demonstrating LSP autocomplete preservation</li> <li><code>example_argument_parser.py</code>: Updated comments to highlight type preservation</li> <li><code>ARGUMENT_PARSER.md</code>: Updated documentation</li> </ul>"},{"location":"LSP_FIX_SUMMARY/#impact","title":"Impact","text":"<p>This change maintains 100% backward compatibility while fixing LSP autocomplete. All existing code continues to work exactly the same, but now with better IDE support!</p>"},{"location":"MP_TQDM/","title":"Progress Bars in Distributed Training (mp_tqdm)","text":"<p>When running distributed training across multiple GPUs, showing progress bars from all processes creates cluttered, overlapping output. ExpMate provides <code>mp_tqdm</code>, a wrapper around tqdm that only displays progress bars on <code>local_rank==0</code> of each node.</p>"},{"location":"MP_TQDM/#overview","title":"Overview","text":"<p><code>mp_tqdm</code> automatically: - Shows progress bars only on local_rank 0 (master process per node) - Hides progress bars on all other processes - Falls back gracefully if tqdm is not installed - Supports all standard tqdm features and parameters - Works seamlessly with both single and multi-GPU setups</p>"},{"location":"MP_TQDM/#installation","title":"Installation","text":"<pre><code># Install tqdm (optional but recommended)\npip install tqdm\n\n# Or install with expmate extras\npip install expmate[tracking]  # Includes tqdm\n</code></pre>"},{"location":"MP_TQDM/#basic-usage","title":"Basic Usage","text":""},{"location":"MP_TQDM/#simple-progress-bar","title":"Simple Progress Bar","text":"<pre><code>from expmate.torch import mp_tqdm\n\n# In distributed training - only shows on local_rank 0\nfor batch in mp_tqdm(dataloader, desc=\"Training\"):\n    loss = train_step(batch)\n</code></pre>"},{"location":"MP_TQDM/#nested-progress-bars","title":"Nested Progress Bars","text":"<p>Track both epochs and batches:</p> <pre><code>from expmate.torch import mp_tqdm\n\nfor epoch in mp_tqdm(range(epochs), desc=\"Epochs\"):\n    for batch in mp_tqdm(dataloader, desc=f\"Epoch {epoch}\", leave=False):\n        loss = train_step(batch)\n</code></pre>"},{"location":"MP_TQDM/#context-manager-for-manual-updates","title":"Context Manager for Manual Updates","text":"<pre><code>from expmate.torch import mp_tqdm\n\nwith mp_tqdm(total=total_steps, desc=\"Processing\") as pbar:\n    for i in range(total_steps):\n        result = process_item(i)\n        pbar.update(1)\n        pbar.set_postfix(loss=result)\n</code></pre>"},{"location":"MP_TQDM/#complete-example","title":"Complete Example","text":"<pre><code>#!/usr/bin/env python3\nfrom expmate.torch import mp_tqdm, setup_ddp, cleanup_ddp\nimport torch\nfrom torch.utils.data import DataLoader\n\ndef train_epoch(model, dataloader, optimizer, device):\n    model.train()\n    total_loss = 0.0\n\n    # Progress bar only shows on local_rank 0\n    for data, target in mp_tqdm(dataloader, desc=\"Training\", leave=False):\n        data, target = data.to(device), target.to(device)\n\n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n\n    return total_loss / len(dataloader)\n\ndef main():\n    # Setup DDP\n    rank, local_rank, world_size = setup_ddp()\n    device = torch.device(f\"cuda:{local_rank}\" if torch.cuda.is_available() else \"cpu\")\n\n    # Create model\n    model = MyModel().to(device)\n    if world_size &gt; 1:\n        model = torch.nn.parallel.DistributedDataParallel(\n            model,\n            device_ids=[local_rank] if torch.cuda.is_available() else None\n        )\n\n    # Training loop with progress tracking\n    for epoch in mp_tqdm(range(num_epochs), desc=\"Epochs\"):\n        train_loss = train_epoch(model, train_loader, optimizer, device)\n\n        # This also prints only on rank 0\n        if rank == 0:\n            print(f\"Epoch {epoch}: loss={train_loss:.4f}\")\n\n    cleanup_ddp()\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>Output (only on local_rank 0): <pre><code>Epochs:   0%|                                      | 0/10 [00:00&lt;?, ?it/s]\nTraining: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 782/782 [00:15&lt;00:00, 51.2it/s]\nEpoch 0: loss=0.4523\nEpochs:  10%|\u2588\u2588\u2588\u2588                                  | 1/10 [00:15&lt;02:15, 15.0s/it]\n</code></pre></p> <p>Output on other processes: <pre><code>Epoch 0: loss=0.4523\nEpoch 1: loss=0.3821\n...\n</code></pre></p>"},{"location":"MP_TQDM/#advanced-features","title":"Advanced Features","text":""},{"location":"MP_TQDM/#manual-control","title":"Manual Control","text":"<p>Override automatic behavior:</p> <pre><code>from expmate.torch import mp_tqdm\n\n# Force enable on all ranks\nfor batch in mp_tqdm(dataloader, disable=False):\n    process(batch)\n\n# Force disable even on rank 0\nfor batch in mp_tqdm(dataloader, disable=True):\n    process(batch)\n</code></pre>"},{"location":"MP_TQDM/#custom-formatting","title":"Custom Formatting","text":"<p>All tqdm parameters are supported:</p> <pre><code>from expmate.torch import mp_tqdm\n\nfor batch in mp_tqdm(\n    dataloader,\n    desc=\"Processing\",\n    unit=\"batches\",\n    ncols=100,\n    leave=True,\n    colour=\"green\",\n):\n    process(batch)\n</code></pre>"},{"location":"MP_TQDM/#dynamic-postfix","title":"Dynamic Postfix","text":"<p>Update progress bar with current metrics:</p> <pre><code>from expmate.torch import mp_tqdm\n\nwith mp_tqdm(dataloader, desc=\"Training\") as pbar:\n    for batch in pbar:\n        loss = train_step(batch)\n        pbar.set_postfix(loss=f\"{loss:.4f}\", lr=optimizer.param_groups[0]['lr'])\n</code></pre>"},{"location":"MP_TQDM/#behavior","title":"Behavior","text":""},{"location":"MP_TQDM/#single-gpu-or-cpu","title":"Single-GPU or CPU","text":"<ul> <li><code>local_rank</code> is always 0</li> <li>Progress bars are shown</li> </ul>"},{"location":"MP_TQDM/#multi-gpu-same-node","title":"Multi-GPU (same node)","text":"<ul> <li>Only <code>local_rank==0</code> shows progress bars</li> <li>Other ranks run silently</li> </ul>"},{"location":"MP_TQDM/#multi-node-multi-gpu","title":"Multi-Node Multi-GPU","text":"<ul> <li>Each node shows progress on its <code>local_rank==0</code></li> <li>Useful for monitoring per-node progress</li> </ul>"},{"location":"MP_TQDM/#fallback-behavior","title":"Fallback Behavior","text":"<p>If tqdm is not installed: <pre><code>from expmate.torch import mp_tqdm\n\n# Returns the iterable directly, no progress bar\nfor item in mp_tqdm(data):\n    process(item)  # Works fine, just no progress bar\n</code></pre></p>"},{"location":"MP_TQDM/#comparison-with-regular-tqdm","title":"Comparison with Regular tqdm","text":"Feature Regular tqdm mp_tqdm Single process \u2705 Shows bar \u2705 Shows bar Multi-GPU (all ranks) \u26a0\ufe0f Cluttered output \u2705 Only local_rank 0 Auto-detection \u274c No \u2705 Yes Fallback if not installed \u274c ImportError \u2705 Graceful All tqdm features \u2705 Yes \u2705 Yes"},{"location":"MP_TQDM/#best-practices","title":"Best Practices","text":""},{"location":"MP_TQDM/#1-use-for-training-loops","title":"1. Use for Training Loops","text":"<pre><code># \u2713 Good: Clear, single progress bar per node\nfor epoch in mp_tqdm(range(epochs)):\n    for batch in mp_tqdm(train_loader, leave=False):\n        train(batch)\n</code></pre>"},{"location":"MP_TQDM/#2-combine-with-rank-aware-logging","title":"2. Combine with Rank-Aware Logging","text":"<pre><code>from expmate.torch import mp_tqdm, mp_print\n\nfor epoch in mp_tqdm(range(epochs)):\n    loss = train_epoch()\n    mp_print(f\"Epoch {epoch}: loss={loss:.4f}\")  # Also only rank 0\n</code></pre>"},{"location":"MP_TQDM/#3-use-leavefalse-for-inner-loops","title":"3. Use <code>leave=False</code> for Inner Loops","text":"<pre><code># Outer loop: keep progress bar\nfor epoch in mp_tqdm(range(epochs), desc=\"Epochs\"):\n    # Inner loop: remove after completion\n    for batch in mp_tqdm(train_loader, desc=\"Batches\", leave=False):\n        train(batch)\n</code></pre>"},{"location":"MP_TQDM/#4-disable-for-debugging","title":"4. Disable for Debugging","text":"<pre><code># During debugging, show on all ranks\nDEBUG = True\n\nfor batch in mp_tqdm(dataloader, disable=DEBUG or None):\n    train(batch)\n</code></pre>"},{"location":"MP_TQDM/#troubleshooting","title":"Troubleshooting","text":""},{"location":"MP_TQDM/#progress-bar-shows-on-all-ranks","title":"Progress bar shows on all ranks","text":"<p>Issue: <code>LOCAL_RANK</code> environment variable not set Solution: Use <code>torchrun</code> or set <code>LOCAL_RANK</code> manually</p> <pre><code># Correct\ntorchrun --nproc_per_node=4 train.py\n\n# Incorrect (missing env vars)\npython train.py  # In multi-GPU setup\n</code></pre>"},{"location":"MP_TQDM/#progress-bar-not-showing-at-all","title":"Progress bar not showing at all","text":"<p>Issue: tqdm not installed Solution: Install tqdm</p> <pre><code>pip install tqdm\n</code></pre>"},{"location":"MP_TQDM/#progress-bar-disappears-too-quickly","title":"Progress bar disappears too quickly","text":"<p>Issue: <code>leave=False</code> on outer loop Solution: Use <code>leave=True</code> (default) for outer loops</p> <pre><code># \u2713 Correct\nfor epoch in mp_tqdm(range(epochs)):  # leave=True by default\n    for batch in mp_tqdm(dataloader, leave=False):  # Clean up after\n        train(batch)\n</code></pre>"},{"location":"MP_TQDM/#see-also","title":"See Also","text":"<ul> <li>Distributed Training Guide</li> <li>Examples: DDP Training</li> </ul>"},{"location":"PACKAGE_FLAGS/","title":"ExpMate Package-Level Configuration Flags","text":"<p>All flags can be configured either programmatically or via environment variables.</p>"},{"location":"PACKAGE_FLAGS/#available-flags","title":"Available Flags","text":""},{"location":"PACKAGE_FLAGS/#1-expmatedebug-default-false","title":"1. <code>expmate.debug</code> (default: <code>False</code>)","text":"<p>Enable debug mode and debug-level logging.</p> <p>Use cases: - Development and debugging - Verbose logging for troubleshooting</p> <p>Configuration: <pre><code>import expmate\nexpmate.debug = True  # Enable debug mode\n</code></pre> <pre><code>export EM_DEBUG=1  # Enable via environment\n</code></pre></p>"},{"location":"PACKAGE_FLAGS/#2-expmatetimer-default-true","title":"2. <code>expmate.timer</code> (default: <code>True</code>)","text":"<p>Control whether <code>logger.timer()</code> performs timing measurements.</p> <p>Use cases: - Disable in production to eliminate profiling overhead - Enable during development to track performance</p> <p>Configuration: <pre><code>import expmate\nexpmate.timer = False  # Disable profiling\n</code></pre> <pre><code>export EM_TIMER=0  # Disable via environment\n</code></pre></p>"},{"location":"PACKAGE_FLAGS/#3-expmatelog_level-default-info","title":"3. <code>expmate.log_level</code> (default: <code>\"INFO\"</code>)","text":"<p>Default log level for all <code>ExperimentLogger</code> instances.</p> <p>Options: <code>\"DEBUG\"</code>, <code>\"INFO\"</code>, <code>\"WARNING\"</code>, <code>\"ERROR\"</code>, <code>\"CRITICAL\"</code></p> <p>Use cases: - Set to <code>\"WARNING\"</code> in production for quieter logs - Set to <code>\"DEBUG\"</code> during development</p> <p>Configuration: <pre><code>import expmate\nexpmate.log_level = \"WARNING\"  # Only warnings and errors\n</code></pre> <pre><code>export EM_LOG_LEVEL=WARNING  # Set via environment\n</code></pre></p>"},{"location":"PACKAGE_FLAGS/#4-expmateverbose-default-true","title":"4. <code>expmate.verbose</code> (default: <code>True</code>)","text":"<p>Control console output for loggers.</p> <p>Use cases: - Disable for cleaner output when running many experiments - Enable for interactive debugging</p> <p>Configuration: <pre><code>import expmate\nexpmate.verbose = False  # Disable console output\n</code></pre> <pre><code>export EM_VERBOSE=0  # Disable via environment\n</code></pre></p>"},{"location":"PACKAGE_FLAGS/#5-expmatetrack_metrics-default-true","title":"5. <code>expmate.track_metrics</code> (default: <code>True</code>)","text":"<p>Control whether to log metrics to CSV files.</p> <p>Use cases: - Disable for faster iteration when metrics aren't needed - Reduce I/O during quick experiments</p> <p>Configuration: <pre><code>import expmate\nexpmate.track_metrics = False  # Skip metrics CSV\n</code></pre> <pre><code>export EM_TRACK_METRICS=0  # Disable via environment\n</code></pre></p>"},{"location":"PACKAGE_FLAGS/#6-expmatetrack_git-default-true","title":"6. <code>expmate.track_git</code> (default: <code>True</code>)","text":"<p>Control whether to collect git repository information.</p> <p>Use cases: - Disable in CI/CD where git operations are slow - Skip when not using version control</p> <p>Configuration: <pre><code>import expmate\nexpmate.track_git = False  # Skip git info\n</code></pre> <pre><code>export EM_TRACK_GIT=0  # Disable via environment\n</code></pre></p>"},{"location":"PACKAGE_FLAGS/#7-expmatesave_checkpoints-default-true","title":"7. <code>expmate.save_checkpoints</code> (default: <code>True</code>)","text":"<p>Control whether to save model checkpoints (PyTorch).</p> <p>Use cases: - Disable for quick experiments or testing - Skip checkpoint I/O during debugging</p> <p>Configuration: <pre><code>import expmate\nexpmate.save_checkpoints = False  # Skip checkpoint saving\n</code></pre> <pre><code>export EM_SAVE_CHECKPOINTS=0  # Disable via environment\n</code></pre></p>"},{"location":"PACKAGE_FLAGS/#8-expmateforce_single_process-default-false","title":"8. <code>expmate.force_single_process</code> (default: <code>False</code>)","text":"<p>Force single-process mode even in distributed environments.</p> <p>Use cases: - Debug distributed code without multiple processes - Run DDP code in single-process mode</p> <p>Configuration: <pre><code>import expmate\nexpmate.force_single_process = True  # Force single process\n</code></pre> <pre><code>export EM_FORCE_SINGLE=1  # Enable via environment\n</code></pre></p>"},{"location":"PACKAGE_FLAGS/#usage-patterns","title":"Usage Patterns","text":""},{"location":"PACKAGE_FLAGS/#production-configuration","title":"Production Configuration","text":"<p>Minimize overhead and noise: <pre><code>import expmate\n\nexpmate.timer = False            # No timing overhead\nexpmate.log_level = \"WARNING\"    # Only warnings/errors\nexpmate.verbose = False          # No console output\n</code></pre></p> <p>Or via environment: <pre><code>export EM_TIMER=0\nexport EM_LOG_LEVEL=WARNING\nexport EM_VERBOSE=0\n</code></pre></p>"},{"location":"PACKAGE_FLAGS/#development-configuration","title":"Development Configuration","text":"<p>Maximum visibility and debugging: <pre><code>import expmate\n\nexpmate.debug = True             # Debug logging\nexpmate.log_level = \"DEBUG\"      # All log messages\nexpmate.timer = True             # Track performance\n</code></pre></p> <p>Or via environment: <pre><code>export EM_DEBUG=1\nexport EM_LOG_LEVEL=DEBUG\nexport EM_TIMER=1\n</code></pre></p>"},{"location":"PACKAGE_FLAGS/#quick-experimentation","title":"Quick Experimentation","text":"<p>Skip I/O for faster iteration: <pre><code>import expmate\n\nexpmate.track_metrics = False    # No CSV writing\nexpmate.save_checkpoints = False # No checkpoint I/O\nexpmate.track_git = False        # No git operations\n</code></pre></p> <p>Or via environment: <pre><code>export EM_TRACK_METRICS=0\nexport EM_SAVE_CHECKPOINTS=0\nexport EM_TRACK_GIT=0\n</code></pre></p>"},{"location":"PACKAGE_FLAGS/#runtime-toggle","title":"Runtime Toggle","text":"<p>Enable/disable features dynamically: <pre><code>import expmate\n\n# Start with profiling disabled\nexpmate.timer = False\n\n# ... fast training loop ...\n\n# Enable timing for critical section\nexpmate.timer = True\nwith logger.timer('critical_section'):\n    important_code()\n\n# Disable again\nexpmate.timer = False\n</code></pre></p>"},{"location":"PACKAGE_FLAGS/#environment-variable-reference","title":"Environment Variable Reference","text":"Flag Environment Variable Default <code>debug</code> <code>EM_DEBUG</code> <code>0</code> (False) <code>timer</code> <code>EM_TIMER</code> <code>1</code> (True) <code>log_level</code> <code>EM_LOG_LEVEL</code> <code>INFO</code> <code>verbose</code> <code>EM_VERBOSE</code> <code>1</code> (True) <code>track_metrics</code> <code>EM_TRACK_METRICS</code> <code>1</code> (True) <code>track_git</code> <code>EM_TRACK_GIT</code> <code>1</code> (True) <code>save_checkpoints</code> <code>EM_SAVE_CHECKPOINTS</code> <code>1</code> (True) <code>force_single_process</code> <code>EM_FORCE_SINGLE</code> <code>0</code> (False)"},{"location":"PACKAGE_FLAGS/#testing","title":"Testing","text":"<p>Run comprehensive tests: <pre><code>python test_package_flags.py\n</code></pre></p> <p>Check current flag values: <pre><code>import expmate\n\nprint(f\"debug: {expmate.debug}\")\nprint(f\"timer: {expmate.timer}\")\nprint(f\"log_level: {expmate.log_level}\")\nprint(f\"verbose: {expmate.verbose}\")\nprint(f\"track_metrics: {expmate.track_metrics}\")\nprint(f\"track_git: {expmate.track_git}\")\nprint(f\"save_checkpoints: {expmate.save_checkpoints}\")\nprint(f\"force_single_process: {expmate.force_single_process}\")\n</code></pre></p>"},{"location":"TYPED_CONFIG/","title":"Typed Configuration with LSP Support","text":"<p>The <code>Config</code> class now supports Python dataclasses for full LSP (Language Server Protocol) support in your IDE!</p>"},{"location":"TYPED_CONFIG/#features","title":"Features","text":"<ul> <li>\u2705 Full IDE autocomplete - Get suggestions for all config fields</li> <li>\u2705 Type checking - Catch type errors before runtime</li> <li>\u2705 Inline documentation - Hover to see field descriptions</li> <li>\u2705 Refactoring support - Safely rename fields across your codebase</li> <li>\u2705 Go-to-definition - Jump to config schema definitions</li> <li>\u2705 Proper type inference - LSP knows the exact type returned from <code>from_file()</code>/<code>from_dict()</code></li> <li>\u2705 Backward compatible - Existing code continues to work</li> </ul>"},{"location":"TYPED_CONFIG/#quick-start","title":"Quick Start","text":""},{"location":"TYPED_CONFIG/#method-1-dynamic-config-original","title":"Method 1: Dynamic Config (Original)","text":"<p>Works like before - no type hints, but very flexible:</p> <pre><code>from expmate.config import Config\n\n# Load config dynamically\nconfig = Config.from_file(\"config.yaml\")\n\n# Access with dot notation (no autocomplete)\nlr = config.training.lr\nepochs = config.training.epochs\n</code></pre>"},{"location":"TYPED_CONFIG/#method-2-typed-config-new-recommended","title":"Method 2: Typed Config (NEW - Recommended!)","text":"<p>Define your config schema for full LSP support:</p> <pre><code>from dataclasses import dataclass\nfrom expmate.config import Config\n\n# Define typed config schema\n@dataclass\nclass TrainingConfig(Config):\n    \"\"\"Training hyperparameters.\"\"\"\n    learning_rate: float\n    batch_size: int\n    epochs: int\n    optimizer: str = \"adam\"  # with default\n\n# Load config with types!\nconfig = TrainingConfig.from_file(\"config.yaml\")\n\n# Full LSP autocomplete and type checking!\nlr: float = config.learning_rate  # IDE knows this is float\nbatch_size: int = config.batch_size  # IDE knows this is int\n</code></pre>"},{"location":"TYPED_CONFIG/#usage-examples","title":"Usage Examples","text":""},{"location":"TYPED_CONFIG/#basic-typed-config","title":"Basic Typed Config","text":"<pre><code>from dataclasses import dataclass\nfrom expmate.config import Config\n\n@dataclass\nclass MyConfig(Config):\n    name: str\n    seed: int\n    learning_rate: float\n    description: str = \"My experiment\"\n\n# Load from YAML\nconfig = MyConfig.from_file(\"config.yaml\")\n\n# Or from dict\nconfig = MyConfig.from_dict({\n    \"name\": \"exp1\",\n    \"seed\": 42,\n    \"learning_rate\": 0.001\n})\n\n# Access with full LSP support\nprint(f\"Experiment: {config.name}\")\nprint(f\"LR: {config.learning_rate}\")\n</code></pre>"},{"location":"TYPED_CONFIG/#nested-typed-config","title":"Nested Typed Config","text":"<pre><code>from dataclasses import dataclass\nfrom typing import Optional\nfrom expmate.config import Config\n\n@dataclass\nclass ModelConfig(Config):\n    \"\"\"Model architecture.\"\"\"\n    hidden_dim: int\n    num_layers: int\n    dropout: float = 0.1\n\n@dataclass\nclass ExperimentConfig(Config):\n    \"\"\"Main experiment config.\"\"\"\n    name: str\n    seed: int\n    # Nested configs can be dict or typed\n    model: dict  # For now, use dict for nested configs\n\nconfig = ExperimentConfig.from_file(\"config.yaml\")\n\n# Top-level fields have full LSP support\nname: str = config.name\nseed: int = config.seed\n\n# Nested dicts accessed normally\nmodel_hidden = config.model[\"hidden_dim\"]\n</code></pre>"},{"location":"TYPED_CONFIG/#mixed-dynamic-and-typed-access","title":"Mixed Dynamic and Typed Access","text":"<pre><code># Even with typed configs, you can still use dynamic access\nconfig = MyConfig.from_file(\"config.yaml\")\n\n# Typed access (preferred)\nname: str = config.name\n\n# Dict-style access (still works)\nname = config[\"name\"]\n\n# Convert to dict\nconfig_dict = config.to_dict()\n\n# Save to file\nconfig.save(\"saved_config.yaml\")\n\n# Generate hash\nhash_val = config.hash()\n</code></pre>"},{"location":"TYPED_CONFIG/#migration-guide","title":"Migration Guide","text":""},{"location":"TYPED_CONFIG/#existing-code-no-changes-needed","title":"Existing Code (No Changes Needed)","text":"<pre><code># This still works exactly as before!\nconfig = Config(\"config.yaml\")\nlr = config.training.lr\n</code></pre>"},{"location":"TYPED_CONFIG/#upgrade-to-typed-config","title":"Upgrade to Typed Config","text":"<ol> <li>Define your schema:</li> </ol> <pre><code>@dataclass\nclass MyConfig(Config):\n    learning_rate: float\n    batch_size: int\n    epochs: int\n</code></pre> <ol> <li>Update initialization:</li> </ol> <pre><code># Old\nconfig = Config(\"config.yaml\")\n\n# New (both work!)\nconfig = Config.from_file(\"config.yaml\")  # Dynamic\nconfig = MyConfig.from_file(\"config.yaml\")  # Typed\n</code></pre> <ol> <li>Enjoy LSP support!</li> </ol> <p>Your IDE will now provide autocomplete, type checking, and documentation for all config fields.</p>"},{"location":"TYPED_CONFIG/#best-practices","title":"Best Practices","text":"<ol> <li>Define schemas for important configs - Use dataclasses for configs you use frequently</li> <li>Use type hints - Annotate variables: <code>lr: float = config.learning_rate</code></li> <li>Add docstrings - Document your config classes and fields</li> <li>Use defaults - Provide sensible defaults: <code>optimizer: str = \"adam\"</code></li> <li>Keep flexibility - Use dynamic <code>Config</code> for one-off experiments</li> </ol>"},{"location":"TYPED_CONFIG/#yaml-config-example","title":"YAML Config Example","text":"<pre><code># config.yaml\nname: my_experiment\nseed: 42\nlearning_rate: 0.001\nbatch_size: 32\nepochs: 100\noptimizer: adam\n</code></pre> <pre><code># Python code\n@dataclass\nclass MyConfig(Config):\n    name: str\n    seed: int\n    learning_rate: float\n    batch_size: int\n    epochs: int\n    optimizer: str\n\nconfig = MyConfig.from_file(\"config.yaml\")\n# All fields autocomplete! \u2728\n</code></pre>"},{"location":"TYPED_CONFIG/#advanced-features","title":"Advanced Features","text":""},{"location":"TYPED_CONFIG/#optional-fields","title":"Optional Fields","text":"<pre><code>from typing import Optional\n\n@dataclass\nclass MyConfig(Config):\n    required_field: str\n    optional_field: Optional[int] = None\n</code></pre>"},{"location":"TYPED_CONFIG/#overrides","title":"Overrides","text":"<pre><code># Works with typed configs too!\nconfig = MyConfig.from_file(\n    \"config.yaml\",\n    overrides=[\"learning_rate=0.01\", \"batch_size=64\"]\n)\n</code></pre>"},{"location":"TYPED_CONFIG/#run-directory-snapshots","title":"Run Directory Snapshots","text":"<pre><code>from pathlib import Path\n\nconfig = MyConfig.from_file(\n    \"config.yaml\",\n    run_dir=Path(\"runs/exp_001\")\n)\n# Automatically saves config snapshot to runs/exp_001/run.yaml\n</code></pre>"},{"location":"TYPED_CONFIG/#troubleshooting","title":"Troubleshooting","text":""},{"location":"TYPED_CONFIG/#config-has-no-attribute-x","title":"\"Config has no attribute X\"","text":"<p>Make sure your YAML keys match your dataclass field names:</p> <pre><code>@dataclass\nclass MyConfig(Config):\n    learning_rate: float  # \u2190 Must match YAML key\n</code></pre> <pre><code>learning_rate: 0.001  # \u2190 Must match dataclass field\n</code></pre>"},{"location":"TYPED_CONFIG/#type-mismatches","title":"Type Mismatches","text":"<p>The config will try to load values as the specified type. Make sure your YAML values match:</p> <pre><code>@dataclass\nclass MyConfig(Config):\n    epochs: int  # YAML should have: epochs: 100 (not \"100\")\n    lr: float    # YAML should have: lr: 0.001 (not 1e-3 notation might work)\n</code></pre>"},{"location":"TYPED_CONFIG/#summary","title":"Summary","text":"<p>The new dataclass-based <code>Config</code> class gives you the best of both worlds:</p> <ul> <li>Flexibility - Dynamic configs for quick experiments</li> <li>Type safety - Typed configs for production code</li> <li>LSP support - Full IDE integration for typed configs</li> <li>Backward compatible - All existing code still works!</li> </ul> <p>Start using typed configs today and enjoy a better development experience! \ud83d\ude80</p>"},{"location":"changelog/","title":"Changelog","text":"<p>All notable changes to this project will be documented in this file.</p> <p>The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.</p>"},{"location":"changelog/#unreleased","title":"Unreleased","text":""},{"location":"changelog/#013-2025-10-29","title":"0.1.3 - 2025-10-29","text":""},{"location":"changelog/#added","title":"Added","text":"<ul> <li>Automatic ETA estimation in <code>log_metric()</code>: Simply pass <code>total_steps</code> parameter to automatically track progress and estimate remaining time</li> <li><code>_format_time()</code> helper method for human-readable time formatting (e.g., \"2h 15m 30s\")</li> <li>New example <code>examples/example_auto_eta.py</code> demonstrating automatic time estimation</li> <li>Comprehensive test suite for automatic ETA feature (<code>tests/test_auto_eta.py</code>)</li> </ul>"},{"location":"changelog/#changed","title":"Changed","text":"<ul> <li>Renamed global flag <code>profile</code> to <code>timer</code> for better consistency with method name</li> <li>Renamed environment variable <code>EM_PROFILE</code> to <code>EM_TIMER</code></li> <li>Updated all documentation and examples to reflect the <code>profile</code> \u2192 <code>timer</code> rename</li> <li>Enhanced <code>log_metric()</code> with optional <code>total_steps</code> and <code>eta_key</code> parameters</li> </ul>"},{"location":"changelog/#fixed","title":"Fixed","text":"<ul> <li>All tests updated to use new <code>timer</code> flag naming</li> </ul>"},{"location":"changelog/#012-2025-10-xx","title":"0.1.2 - 2025-10-XX","text":""},{"location":"changelog/#added_1","title":"Added","text":"<ul> <li>Documentation site with MkDocs</li> <li>Comprehensive README for PyPI</li> </ul>"},{"location":"changelog/#010-2025-01-xx","title":"0.1.0 - 2025-01-XX","text":""},{"location":"changelog/#added_2","title":"Added","text":"<ul> <li>Initial release</li> <li>Configuration management with YAML and CLI overrides</li> <li>Experiment logging with structured metrics</li> <li>Checkpoint management for PyTorch models</li> <li>Distributed training utilities (DDP support)</li> <li>WandB and TensorBoard integration</li> <li>CLI tools for comparing runs and visualizing metrics</li> <li>Git integration for reproducibility</li> <li>Hyperparameter sweep utilities</li> <li>Examples and documentation</li> </ul>"},{"location":"changelog/#features","title":"Features","text":"<ul> <li>Hierarchical configuration with dot notation</li> <li>Variable interpolation in configs</li> <li>Rank-aware logging for distributed training</li> <li>Best model tracking</li> <li>Automatic checkpoint cleanup</li> <li>DDP-safe run directory creation</li> <li>Metrics visualization</li> <li>Run comparison tools</li> </ul>"},{"location":"contributing/","title":"Contributing to ExpMate","text":"<p>Thank you for your interest in contributing to ExpMate! This guide will help you get started.</p>"},{"location":"contributing/#development-setup","title":"Development Setup","text":""},{"location":"contributing/#1-fork-and-clone","title":"1. Fork and Clone","text":"<pre><code>git clone https://github.com/YOUR_USERNAME/expmate.git\ncd expmate\n</code></pre>"},{"location":"contributing/#2-install-development-dependencies","title":"2. Install Development Dependencies","text":"<pre><code>pip install -e \".[dev]\"\n</code></pre> <p>This installs ExpMate in editable mode along with development tools: - pytest: Testing framework - pytest-cov: Coverage reporting - black: Code formatting - ruff: Linting - mypy: Type checking - pre-commit: Git hooks</p>"},{"location":"contributing/#3-install-pre-commit-hooks","title":"3. Install Pre-commit Hooks","text":"<pre><code>pre-commit install\n</code></pre> <p>This sets up automatic code formatting and linting on commit.</p>"},{"location":"contributing/#development-workflow","title":"Development Workflow","text":""},{"location":"contributing/#1-create-a-branch","title":"1. Create a Branch","text":"<pre><code>git checkout -b feature/your-feature-name\n</code></pre> <p>Use descriptive branch names: - <code>feature/add-new-parser</code> - <code>fix/checkpoint-bug</code> - <code>docs/update-readme</code></p>"},{"location":"contributing/#2-make-your-changes","title":"2. Make Your Changes","text":"<p>Write clean, well-documented code following the existing style.</p>"},{"location":"contributing/#3-write-tests","title":"3. Write Tests","text":"<p>Add tests for your changes in the <code>tests/</code> directory:</p> <pre><code># tests/test_your_feature.py\nimport pytest\nfrom expmate import YourFeature\n\ndef test_your_feature():\n    feature = YourFeature()\n    assert feature.works() == True\n</code></pre>"},{"location":"contributing/#4-run-tests","title":"4. Run Tests","text":"<pre><code># Run all tests\npytest\n\n# Run with coverage\npytest --cov=expmate\n\n# Run specific test file\npytest tests/test_config.py\n\n# Run specific test\npytest tests/test_config.py::test_load_config\n</code></pre>"},{"location":"contributing/#5-format-code","title":"5. Format Code","text":"<pre><code># Format with black\nblack src/expmate tests\n\n# Lint with ruff\nruff check src/expmate tests\n\n# Type check with mypy\nmypy src/expmate\n</code></pre>"},{"location":"contributing/#6-commit-changes","title":"6. Commit Changes","text":"<pre><code>git add .\ngit commit -m \"Add feature: descriptive message\"\n</code></pre> <p>Pre-commit hooks will automatically format and lint your code.</p>"},{"location":"contributing/#7-push-and-create-pr","title":"7. Push and Create PR","text":"<pre><code>git push origin feature/your-feature-name\n</code></pre> <p>Then create a Pull Request on GitHub.</p>"},{"location":"contributing/#code-style","title":"Code Style","text":""},{"location":"contributing/#python-style-guide","title":"Python Style Guide","text":"<ul> <li>Follow PEP 8</li> <li>Use black for formatting (line length: 88)</li> <li>Use type hints for function signatures</li> <li>Write docstrings in Google style</li> </ul> <p>Example:</p> <pre><code>def load_config(\n    config_input: Union[str, List[str], Dict[str, Any]], \n    overrides: List[str] = None\n) -&gt; Dict[str, Any]:\n    \"\"\"Load and merge configuration from YAML files or dict.\n\n    Args:\n        config_input: Config file path(s) or dict\n        overrides: List of key=value overrides\n\n    Returns:\n        Loaded and merged configuration\n\n    Raises:\n        FileNotFoundError: If config file doesn't exist\n        ValueError: If config format is invalid\n    \"\"\"\n    pass\n</code></pre>"},{"location":"contributing/#documentation","title":"Documentation","text":"<ul> <li>Add docstrings to all public functions and classes</li> <li>Include examples in docstrings when helpful</li> <li>Update relevant documentation in <code>docs/</code></li> </ul>"},{"location":"contributing/#testing-guidelines","title":"Testing Guidelines","text":""},{"location":"contributing/#test-coverage","title":"Test Coverage","text":"<ul> <li>Aim for &gt;80% test coverage</li> <li>Test both success and failure cases</li> <li>Include edge cases</li> </ul>"},{"location":"contributing/#test-organization","title":"Test Organization","text":"<pre><code>tests/\n\u251c\u2500\u2500 test_config.py      # Config management tests\n\u251c\u2500\u2500 test_logger.py      # Logging tests\n\u251c\u2500\u2500 test_checkpoint.py  # Checkpoint tests\n\u2514\u2500\u2500 test_integration.py # Integration tests\n</code></pre>"},{"location":"contributing/#test-naming","title":"Test Naming","text":"<p>Use descriptive test names:</p> <pre><code>def test_load_config_from_yaml_file():\n    \"\"\"Test loading config from YAML file.\"\"\"\n    pass\n\ndef test_load_config_with_overrides():\n    \"\"\"Test loading config with CLI overrides.\"\"\"\n    pass\n\ndef test_load_config_raises_on_missing_file():\n    \"\"\"Test that load_config raises FileNotFoundError.\"\"\"\n    pass\n</code></pre>"},{"location":"contributing/#documentation_1","title":"Documentation","text":""},{"location":"contributing/#building-docs-locally","title":"Building Docs Locally","text":"<pre><code># Install docs dependencies\npip install mkdocs mkdocs-material mkdocstrings[python]\n\n# Serve docs locally\nmkdocs serve\n\n# Build docs\nmkdocs build\n</code></pre> <p>Visit http://localhost:8000 to view documentation.</p>"},{"location":"contributing/#documentation-structure","title":"Documentation Structure","text":"<pre><code>docs/\n\u251c\u2500\u2500 index.md                    # Home page\n\u251c\u2500\u2500 getting-started/\n\u2502   \u251c\u2500\u2500 installation.md\n\u2502   \u251c\u2500\u2500 quickstart.md\n\u2502   \u2514\u2500\u2500 concepts.md\n\u251c\u2500\u2500 guide/\n\u2502   \u251c\u2500\u2500 configuration.md\n\u2502   \u251c\u2500\u2500 logging.md\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 api/\n\u2502   \u251c\u2500\u2500 config.md\n\u2502   \u251c\u2500\u2500 logger.md\n\u2502   \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 examples/\n    \u2514\u2500\u2500 ...\n</code></pre>"},{"location":"contributing/#pull-request-guidelines","title":"Pull Request Guidelines","text":""},{"location":"contributing/#pr-checklist","title":"PR Checklist","text":"<ul> <li>[ ] Tests pass (<code>pytest</code>)</li> <li>[ ] Code is formatted (<code>black</code>)</li> <li>[ ] Code is linted (<code>ruff</code>)</li> <li>[ ] Type hints are correct (<code>mypy</code>)</li> <li>[ ] Documentation is updated</li> <li>[ ] CHANGELOG.md is updated</li> <li>[ ] Commit messages are descriptive</li> </ul>"},{"location":"contributing/#pr-description","title":"PR Description","text":"<p>Include in your PR description: - What changes you made - Why you made them - How to test them - Any breaking changes</p> <p>Example:</p> <pre><code>## Description\nAdd support for remote config files via HTTP/HTTPS URLs.\n\n## Motivation\nUsers requested the ability to load configs from remote locations.\n\n## Changes\n- Add `load_config_from_url()` function\n- Update `load_config()` to detect and handle URLs\n- Add tests for URL loading\n- Update documentation\n\n## Testing\n```bash\npytest tests/test_config.py::test_load_config_from_url\n</code></pre>"},{"location":"contributing/#breaking-changes","title":"Breaking Changes","text":"<p>None ```</p>"},{"location":"contributing/#release-process","title":"Release Process","text":"<p>Releases are managed by maintainers:</p> <ol> <li>Update version in <code>pyproject.toml</code> and <code>src/expmate/__init__.py</code></li> <li>Update <code>CHANGELOG.md</code></li> <li>Create git tag: <code>git tag v0.2.0</code></li> <li>Push tag: <code>git push origin v0.2.0</code></li> <li>Build and publish to PyPI</li> </ol>"},{"location":"contributing/#getting-help","title":"Getting Help","text":"<ul> <li>Questions: Open a GitHub Discussion</li> <li>Bugs: Open a GitHub Issue</li> <li>Chat: Join our community (link TBD)</li> </ul>"},{"location":"contributing/#code-of-conduct","title":"Code of Conduct","text":"<p>Be respectful and inclusive. We follow the Contributor Covenant.</p>"},{"location":"contributing/#license","title":"License","text":"<p>By contributing, you agree that your contributions will be licensed under the MIT License.</p>"},{"location":"api/checkpoint/","title":"API Reference: Checkpoint","text":""},{"location":"api/checkpoint/#checkpointmanager","title":"CheckpointManager","text":"<p>Manages model checkpoints with automatic cleanup and best model tracking.</p> <pre><code>from expmate.torch import CheckpointManager\n\nmanager = CheckpointManager(\n    checkpoint_dir='checkpoints',\n    keep_last=3,\n    keep_best=5,\n    metric_name='val_loss',\n    mode='min'\n)\n</code></pre>"},{"location":"api/checkpoint/#methods","title":"Methods","text":""},{"location":"api/checkpoint/#__init__checkpoint_dir-keep_last3-keep_best3-metric_nameloss-modemin","title":"<code>__init__(checkpoint_dir, keep_last=3, keep_best=3, metric_name='loss', mode='min')</code>","text":"<p>Initialize checkpoint manager.</p> <p>Parameters: - <code>checkpoint_dir</code> (str): Directory to save checkpoints - <code>keep_last</code> (int): Number of last checkpoints to keep - <code>keep_best</code> (int): Number of best checkpoints to keep - <code>metric_name</code> (str): Metric name for tracking best checkpoints - <code>mode</code> (str): 'min' or 'max' - whether lower or higher is better</p>"},{"location":"api/checkpoint/#savemodel-optimizernone-schedulernone-epochnone-stepnone-metricsnone-extranone-filenamenone","title":"<code>save(model, optimizer=None, scheduler=None, epoch=None, step=None, metrics=None, extra=None, filename=None)</code>","text":"<p>Save a checkpoint.</p> <p>Returns: Path to saved checkpoint file</p>"},{"location":"api/checkpoint/#load_latest","title":"<code>load_latest()</code>","text":"<p>Load the most recent checkpoint.</p> <p>Returns: Checkpoint dictionary</p>"},{"location":"api/checkpoint/#load_best","title":"<code>load_best()</code>","text":"<p>Load the best checkpoint based on tracked metric.</p> <p>Returns: Checkpoint dictionary</p> <p>See the source code for full API details.</p>"},{"location":"api/config/","title":"API Reference: Config","text":""},{"location":"api/config/#config-class","title":"Config Class","text":"<p>A dictionary-like configuration object with attribute access.</p> <pre><code>from expmate import Config\n\nconfig = Config({'model': {'hidden_dim': 256}})\nprint(config.model.hidden_dim)  # 256\n</code></pre>"},{"location":"api/config/#load_config","title":"load_config()","text":"<p>Load configuration from YAML files or dictionaries.</p> <pre><code>from expmate.config import load_config\n\nconfig = load_config('config.yaml')\nconfig = load_config('config.yaml', overrides=['training.lr=0.01'])\n</code></pre> <p>Parameters: - <code>config_input</code> (str|list|dict): Config file path(s) or dict - <code>overrides</code> (list): List of key=value overrides</p> <p>Returns: Dictionary with configuration</p> <p>See the source code for full API details.</p>"},{"location":"api/logger/","title":"API Reference: Logger","text":""},{"location":"api/logger/#experimentlogger","title":"ExperimentLogger","text":"<p>Structured logging and metrics tracking for experiments with colorful console output, timing, and hierarchical stage tracking.</p> <pre><code>from expmate import ExperimentLogger\n\nlogger = ExperimentLogger(run_dir='runs/exp1')\nlogger.info(\"Training started\")\nlogger.log_metric(step=0, split='train', name='loss', value=0.5)\n</code></pre>"},{"location":"api/logger/#methods","title":"Methods","text":""},{"location":"api/logger/#__init__run_dir-rank0-run_idnone-log_levelinfo-console_outputtrue","title":"<code>__init__(run_dir, rank=0, run_id=None, log_level='INFO', console_output=True)</code>","text":"<p>Initialize experiment logger.</p> <p>Features: - Colorful console output (automatically disabled when output is redirected) - File logging without colors (clean, parseable logs) - JSONL event logging - Metrics tracking with CSV storage</p>"},{"location":"api/logger/#infomessage-warningmessage-errormessage-debugmessage","title":"<code>info(message)</code>, <code>warning(message)</code>, <code>error(message)</code>, <code>debug(message)</code>","text":"<p>Log text messages at different levels with color-coded console output.</p> <p>Console Colors: - <code>DEBUG</code>: Cyan - <code>INFO</code>: Green - <code>WARNING</code>: Yellow - <code>ERROR</code>: Red</p>"},{"location":"api/logger/#log_metricstep-split-name-value-track_besttrue-modenone","title":"<code>log_metric(step, split, name, value, track_best=True, mode=None)</code>","text":"<p>Log a metric value.</p> <p>Parameters: - <code>step</code> (int): Training step/epoch - <code>split</code> (str): Data split ('train', 'val', 'test') - <code>name</code> (str): Metric name - <code>value</code> (float): Metric value - <code>track_best</code> (bool): Whether to track best value - <code>mode</code> (str): 'min' or 'max' for best tracking (auto-detected if None)</p>"},{"location":"api/logger/#timername-log_resulttrue","title":"<code>timer(name, log_result=True)</code>","text":"<p>Context manager for timing code sections (simple wall-clock timing).</p> <pre><code># Basic timing\nwith logger.timer('data_loading'):\n    data = load_data()\n\n# Get elapsed time\nwith logger.timer('training_step') as result:\n    loss = model(batch)\nprint(f\"Step took {result['elapsed']:.4f}s\")\n\n# Silent timing (no logging)\nwith logger.timer('forward', log_result=False) as result:\n    output = model(input)\n</code></pre> <p>Parameters: - <code>name</code> (str): Name of the timed section - <code>log_result</code> (bool): Whether to log the result</p> <p>Yields: - <code>dict</code>: Dictionary with <code>'elapsed'</code> key (in seconds)</p> <p>Note: For detailed GPU/CPU profiling, use <code>torch.profiler</code> directly.</p>"},{"location":"api/logger/#stagename-metadata","title":"<code>stage(name, **metadata)</code>","text":"<p>Context manager for hierarchical stage tracking with duration logging.</p> <pre><code># Simple stage\nwith logger.stage('training'):\n    train_model()\n\n# With metadata\nwith logger.stage('epoch', epoch=5, lr=0.001):\n    train_epoch()\n\n# Nested stages\nwith logger.stage('epoch', epoch=5):\n    with logger.stage('train'):\n        train_loss = train_epoch()\n    with logger.stage('validation'):\n        val_loss = validate()\n</code></pre> <p>Parameters: - <code>name</code> (str): Stage name - <code>**metadata</code>: Additional metadata (e.g., epoch=5, batch=10)</p> <p>Yields: - <code>dict</code>: Stage info with <code>'name'</code>, <code>'metadata'</code>, <code>'elapsed'</code> keys</p> <p>Console Output: Blue color with bold stage names</p>"},{"location":"api/logger/#log_everyeverynone-secondsnone-keynone","title":"<code>log_every(every=None, seconds=None, key=None)</code>","text":"<p>Context manager for rate-limited logging to reduce console spam.</p> <pre><code># Log every 100 iterations\nfor step in range(10000):\n    loss = train_step()\n    with logger.log_every(every=100):\n        logger.info(f\"Step {step}: loss={loss:.4f}\")\n\n# Log every 5 seconds\nfor batch in dataloader:\n    with logger.log_every(seconds=5.0):\n        logger.info(f\"Processing batch...\")\n\n# Multiple rate limiters\nfor step in range(1000):\n    with logger.log_every(every=10, key='loss'):\n        logger.info(f\"Loss: {loss:.4f}\")\n    with logger.log_every(every=100, key='detailed'):\n        logger.info(f\"Detailed metrics: {metrics}\")\n</code></pre> <p>Parameters: - <code>every</code> (int): Log every N iterations - <code>seconds</code> (float): Log every N seconds - <code>key</code> (str): Unique key for this rate limiter (auto-generated if None)</p> <p>Yields: - <code>bool</code>: True if logging should occur, False if suppressed</p> <p>Note: Must specify either <code>every</code> or <code>seconds</code>, not both.</p>"},{"location":"api/logger/#profilename-deprecated","title":"<code>profile(name)</code> (Deprecated)","text":"<p>Deprecated alias for <code>timer()</code>. Use <code>timer()</code> instead.</p>"},{"location":"api/logger/#get_best_metricname-splitval","title":"<code>get_best_metric(name, split='val')</code>","text":"<p>Get the best value for a metric.</p> <p>Parameters: - <code>name</code> (str): Metric name - <code>split</code> (str): Data split</p> <p>Returns: - <code>dict</code> or <code>None</code>: Best metric info with <code>'value'</code>, <code>'step'</code>, <code>'mode'</code> keys</p>"},{"location":"api/logger/#color-output","title":"Color Output","text":"<p>Console output is automatically colorized when connected to a terminal (TTY). Colors are disabled when: - Output is redirected to a file - Output is piped to another command - <code>sys.stdout.isatty()</code> returns False</p> <p>File logs (<code>.log</code> files) and JSONL event logs always use plain text without color codes.</p> <p>See the source code for full API details.</p>"},{"location":"api/parser/","title":"API Reference: Parser","text":""},{"location":"api/parser/#parse_config","title":"parse_config()","text":"<p>Parse configuration from command-line arguments.</p> <pre><code>from expmate import parse_config\n\n# Automatically parses: python script.py config.yaml key=value\nconfig = parse_config()\n</code></pre>"},{"location":"api/parser/#configargumentparser","title":"ConfigArgumentParser","text":"<p>Argument parser with config support.</p> <pre><code>from expmate import ConfigArgumentParser\n\nparser = ConfigArgumentParser()\nparser.add_argument('--gpu', type=int, default=0)\nconfig = parser.parse_config()\n</code></pre>"},{"location":"api/parser/#parse_value","title":"parse_value()","text":"<p>Parse string value to appropriate type.</p> <pre><code>from expmate.parser import parse_value\n\nparse_value('42')      # 42 (int)\nparse_value('3.14')    # 3.14 (float)\nparse_value('true')    # True (bool)\nparse_value('[1,2,3]') # [1, 2, 3] (list)\n</code></pre> <p>See the source code for full API details.</p>"},{"location":"api/tracking/","title":"API Reference: Tracking","text":""},{"location":"api/tracking/#wandb-tracker","title":"WandB Tracker","text":"<p>Integration with Weights &amp; Biases.</p> <pre><code>from expmate.tracking import WandbTracker\n\ntracker = WandbTracker(\n    project=\"my-project\",\n    name=\"exp1\",\n    config=config.to_dict()\n)\n\ntracker.log({'train/loss': loss}, step=epoch)\ntracker.finish()\n</code></pre>"},{"location":"api/tracking/#methods","title":"Methods","text":""},{"location":"api/tracking/#__init__project-namenone-confignone-kwargs","title":"<code>__init__(project, name=None, config=None, **kwargs)</code>","text":"<p>Initialize WandB tracker.</p>"},{"location":"api/tracking/#logmetrics-stepnone","title":"<code>log(metrics, step=None)</code>","text":"<p>Log metrics to WandB.</p>"},{"location":"api/tracking/#log_artifactpath-name-artifact_typemodel","title":"<code>log_artifact(path, name, artifact_type='model')</code>","text":"<p>Log a file artifact.</p>"},{"location":"api/tracking/#finish","title":"<code>finish()</code>","text":"<p>Finish the WandB run.</p>"},{"location":"api/tracking/#tensorboard-tracker","title":"TensorBoard Tracker","text":"<p>Integration with TensorBoard.</p> <pre><code>from expmate.tracking import TensorBoardTracker\n\ntracker = TensorBoardTracker(log_dir='runs/exp1/tb')\ntracker.log({'loss': loss}, step=epoch)\n</code></pre>"},{"location":"api/tracking/#methods_1","title":"Methods","text":""},{"location":"api/tracking/#__init__log_dir","title":"<code>__init__(log_dir)</code>","text":"<p>Initialize TensorBoard tracker.</p>"},{"location":"api/tracking/#logmetrics-step","title":"<code>log(metrics, step)</code>","text":"<p>Log metrics to TensorBoard.</p>"},{"location":"api/tracking/#log_histogramtag-values-step","title":"<code>log_histogram(tag, values, step)</code>","text":"<p>Log histogram to TensorBoard.</p> <p>See the source code for full API details.</p>"},{"location":"api/utils/","title":"API Reference: Utils","text":""},{"location":"api/utils/#set_seed","title":"set_seed()","text":"<p>Set random seed for reproducibility.</p> <pre><code>from expmate import set_seed\n\nset_seed(42)  # Sets seed for Python, NumPy, and PyTorch\n</code></pre> <p>Parameters: - <code>seed</code> (int): Random seed value</p>"},{"location":"api/utils/#get_gpu_devices","title":"get_gpu_devices()","text":"<p>Get available GPU devices.</p> <pre><code>from expmate import get_gpu_devices\n\ndevices = get_gpu_devices()\nprint(f\"Available GPUs: {devices}\")\n</code></pre> <p>Returns: List of GPU device IDs</p> <p>See the source code for full API details.</p>"},{"location":"examples/ddp/","title":"DDP Training Example","text":"<p>(Documentation in progress - will be added in next update)</p> <p>See the full example: examples/train.py</p>"},{"location":"examples/minimal/","title":"Minimal Example","text":"<p>This example demonstrates the simplest way to use ExpMate with typed configuration, experiment logging, and metrics tracking.</p>"},{"location":"examples/minimal/#source-code","title":"Source Code","text":"<p>The complete example is available at <code>examples/00_minimal.py</code>.</p>"},{"location":"examples/minimal/#quick-start","title":"Quick Start","text":"<pre><code># Run with defaults\npython examples/00_minimal.py\n\n# Run with config file\npython examples/00_minimal.py examples/conf/example.yaml\n\n# Run with config file and overrides\npython examples/00_minimal.py examples/conf/example.yaml +training.lr=0.01 +training.epochs=10\n</code></pre>"},{"location":"examples/minimal/#configuration","title":"Configuration","text":""},{"location":"examples/minimal/#using-typed-config-recommended","title":"Using Typed Config (Recommended)","text":"<pre><code>from dataclasses import dataclass, field\nfrom expmate import Config\n\n@dataclass\nclass ModelConfig(Config):\n    \"\"\"Model configuration\"\"\"\n    input_dim: int = 10\n    output_dim: int = 3\n\n@dataclass\nclass TrainingConfig(Config):\n    \"\"\"Training configuration\"\"\"\n    epochs: int = 5\n    lr: float = 0.001\n\n@dataclass\nclass ExperimentConfig(Config):\n    \"\"\"Main experiment configuration\"\"\"\n    run_id: str = \"minimal_${now:%Y%m%d_%H%M%S}\"\n    seed: int = 42\n    model: ModelConfig = field(default_factory=ModelConfig)\n    training: TrainingConfig = field(default_factory=TrainingConfig)\n</code></pre>"},{"location":"examples/minimal/#optional-yaml-config-file","title":"Optional: YAML Config File","text":"<p>Create <code>examples/conf/example.yaml</code>:</p> <pre><code>run_id: \"exp_${now:%Y%m%d_%H%M%S}\"\nseed: 42\n\nmodel:\n  input_dim: 10\n  output_dim: 3\n\ntraining:\n  epochs: 5\n  lr: 0.001\n</code></pre>"},{"location":"examples/minimal/#training-script","title":"Training Script","text":"<pre><code>#!/usr/bin/env python3\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, TensorDataset\n\nfrom expmate import ExperimentLogger, Config, set_seed\n\n\ndef main():\n    # Parse config from file if provided, otherwise use defaults\n    import sys\n    if len(sys.argv) &gt; 1 and not sys.argv[1].startswith('+'):\n        config_file = sys.argv[1]\n        overrides = sys.argv[2:]\n    else:\n        config_file = None\n        overrides = sys.argv[1:]\n\n    config = ExperimentConfig.from_args(config_file, overrides) if config_file else ExperimentConfig.from_args(overrides=overrides)\n\n    # Set random seed for reproducibility\n    set_seed(config.seed)\n\n    # Create experiment logger\n    logger = ExperimentLogger(run_dir=f\"runs/{config.run_id}\")\n    logger.info(f\"Starting experiment: {config.run_id}\")\n\n    # Create model and data\n    model = nn.Linear(config.model.input_dim, config.model.output_dim)\n    dataset = TensorDataset(\n        torch.randn(100, config.model.input_dim),\n        torch.randint(0, config.model.output_dim, (100,)),\n    )\n    dataloader = DataLoader(dataset, batch_size=32)\n    optimizer = torch.optim.Adam(model.parameters(), lr=config.training.lr)\n    criterion = nn.CrossEntropyLoss()\n\n    # Training loop\n    for epoch in range(config.training.epochs):\n        total_loss = 0\n        for batch_x, batch_y in dataloader:\n            output = model(batch_x)\n            loss = criterion(output, batch_y)\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n            total_loss += loss.item()\n\n        avg_loss = total_loss / len(dataloader)\n        logger.log_metric(step=epoch, split=\"train\", name=\"loss\", value=avg_loss)\n        logger.info(f\"Epoch {epoch}/{config.training.epochs}: loss={avg_loss:.4f}\")\n\n    logger.info(f\"Logs saved to: {logger.run_dir}\")\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"examples/minimal/#running-the-example","title":"Running the Example","text":""},{"location":"examples/minimal/#basic-run-no-config-file","title":"Basic Run (No Config File)","text":"<pre><code>python examples/00_minimal.py\n</code></pre>"},{"location":"examples/minimal/#with-config-file","title":"With Config File","text":"<pre><code>python examples/00_minimal.py examples/conf/example.yaml\n</code></pre>"},{"location":"examples/minimal/#with-parameter-overrides","title":"With Parameter Overrides","text":"<pre><code># Change learning rate\npython examples/00_minimal.py examples/conf/example.yaml +training.lr=0.01\n\n# Change multiple parameters\npython examples/minimal.py examples/conf/example.yaml training.lr=0.01 training.epochs=10\n\n# Add new parameters\npython examples/minimal.py examples/conf/example.yaml +optimizer.weight_decay=0.0001\n</code></pre>"},{"location":"examples/minimal/#output","title":"Output","text":"<p>ExpMate creates a structured run directory:</p> <pre><code>runs/\n\u2514\u2500\u2500 exp_20250123_143022/\n    \u251c\u2500\u2500 run.yaml          # Saved configuration\n    \u251c\u2500\u2500 exp.log           # Training logs\n    \u251c\u2500\u2500 events.jsonl      # Structured events\n    \u251c\u2500\u2500 metrics.csv       # Training metrics\n    \u2514\u2500\u2500 best.json         # Best metric values\n</code></pre>"},{"location":"examples/minimal/#viewing-logs","title":"Viewing Logs","text":"<pre><code># View training logs\ncat runs/exp_20250123_143022/exp.log\n\n# View metrics\ncat runs/exp_20250123_143022/metrics.csv\n</code></pre>"},{"location":"examples/minimal/#comparing-runs","title":"Comparing Runs","text":"<pre><code># Run multiple experiments\npython examples/minimal.py examples/conf/example.yaml training.lr=0.001\npython examples/minimal.py examples/conf/example.yaml training.lr=0.01\npython examples/minimal.py examples/conf/example.yaml training.lr=0.1\n\n# Compare results\nexpmate compare runs/exp_*\n</code></pre>"},{"location":"examples/minimal/#key-concepts-demonstrated","title":"Key Concepts Demonstrated","text":"<ol> <li>Configuration Management: Loading and overriding config values</li> <li>Experiment Logging: Structured logging with automatic file management</li> <li>Metrics Tracking: Logging and tracking training metrics</li> <li>Reproducibility: Setting seeds and saving full configuration</li> </ol>"},{"location":"examples/minimal/#next-steps","title":"Next Steps","text":"<ul> <li>Complete Training Example</li> <li>Distributed Training Example</li> <li>Configuration Guide</li> </ul>"},{"location":"examples/sweeps/","title":"Hyperparameter Sweeps","text":"<p>(Documentation in progress - will be added in next update)</p> <p>For now, see the CLI Tools Guide.</p>"},{"location":"examples/training/","title":"Complete Training Example","text":"<p>(Documentation in progress - will be added in next update)</p> <p>See the full example: examples/train.py</p>"},{"location":"getting-started/concepts/","title":"Basic Concepts","text":"<p>This guide explains the core concepts of ExpMate.</p>"},{"location":"getting-started/concepts/#configuration-management","title":"Configuration Management","text":"<p>ExpMate uses a hierarchical configuration system based on YAML files with command-line overrides.</p>"},{"location":"getting-started/concepts/#config-object","title":"Config Object","text":"<p>The <code>Config</code> object provides dictionary-like and attribute-like access:</p> <pre><code>from expmate import Config\n\nconfig = Config({\n    'model': {'hidden_dim': 256},\n    'training': {'lr': 0.001}\n})\n\n# Dictionary-style access\nprint(config['model']['hidden_dim'])  # 256\n\n# Attribute-style access (preferred)\nprint(config.model.hidden_dim)  # 256\nprint(config.training.lr)  # 0.001\n</code></pre>"},{"location":"getting-started/concepts/#command-line-overrides","title":"Command-Line Overrides","text":"<p>Override any config value from the command line:</p> <pre><code># Simple override\npython train.py config.yaml training.lr=0.01\n\n# Nested values\npython train.py config.yaml model.hidden_dim=512\n\n# Multiple overrides\npython train.py config.yaml training.lr=0.01 training.epochs=50\n\n# Add new keys with + prefix\npython train.py config.yaml +optimizer.weight_decay=0.0001\n\n# Type hints for ambiguous values\npython train.py config.yaml training.lr:float=1e-3\n</code></pre>"},{"location":"getting-started/concepts/#variable-interpolation","title":"Variable Interpolation","text":"<p>Use variables in your config files:</p> <pre><code>run_id: \"exp_${now:%Y%m%d_%H%M%S}\"  # Current timestamp\nseed: 42\noutput_dir: \"outputs/${run_id}\"     # Reference other values\n</code></pre>"},{"location":"getting-started/concepts/#experiment-logging","title":"Experiment Logging","text":"<p>ExpMate provides structured logging for both text and metrics.</p>"},{"location":"getting-started/concepts/#text-logging","title":"Text Logging","text":"<p>Use level-specific methods (preferred):</p> <pre><code>logger.info(\"Training started\")\nlogger.warning(\"Learning rate might be too high\")\nlogger.error(\"NaN detected in loss\")\nlogger.debug(\"Batch processing time: 0.5s\")\n</code></pre> <p>Or the generic method:</p> <pre><code>logger.log(\"Training started\", level=\"INFO\")\n</code></pre>"},{"location":"getting-started/concepts/#metrics-logging","title":"Metrics Logging","text":"<p>Log structured metrics for tracking and visualization:</p> <pre><code>logger.log_metric(\n    step=epoch,\n    split='train',  # 'train', 'val', 'test', etc.\n    name='loss',\n    value=0.5\n)\n</code></pre> <p>All metrics are automatically saved to <code>metrics.csv</code> for easy analysis.</p>"},{"location":"getting-started/concepts/#best-metrics-tracking","title":"Best Metrics Tracking","text":"<p>Track best values for important metrics:</p> <pre><code># Configure tracking (mode: 'min' or 'max')\nlogger.track_best('val_loss', mode='min')\nlogger.track_best('val_accuracy', mode='max')\n\n# Log metrics normally\nlogger.log_metric(step=epoch, split='val', name='loss', value=0.5)\n\n# Best values are automatically tracked and saved to best.json\n</code></pre>"},{"location":"getting-started/concepts/#distributed-training","title":"Distributed Training","text":"<p>ExpMate provides utilities for PyTorch distributed training.</p>"},{"location":"getting-started/concepts/#setup-ddp","title":"Setup DDP","text":"<pre><code>from expmate.torch import mp\n\n# Initialize DDP\nrank, local_rank, world_size = mp.setup_ddp()\n\n# Check if main process\nis_main = rank == 0\n</code></pre>"},{"location":"getting-started/concepts/#rank-aware-logging","title":"Rank-Aware Logging","text":"<pre><code>from expmate import ExperimentLogger\n\n# Pass rank to logger\nlogger = ExperimentLogger(run_dir=run_dir, rank=rank)\n\n# Only rank 0 writes to main log files\nlogger.info(\"This message appears in all rank logs\")\n</code></pre>"},{"location":"getting-started/concepts/#shared-run-directory","title":"Shared Run Directory","text":"<p>Create a single run directory shared across all processes:</p> <pre><code># DDP-safe directory creation\nrun_dir = mp.create_shared_run_dir(\n    base_dir=\"runs\",\n    run_id=config.run_id\n)\n</code></pre>"},{"location":"getting-started/concepts/#checkpoint-management","title":"Checkpoint Management","text":"<p>Manage model checkpoints with automatic cleanup and best model tracking.</p>"},{"location":"getting-started/concepts/#basic-usage","title":"Basic Usage","text":"<pre><code>from expmate.torch import CheckpointManager\n\nmanager = CheckpointManager(\n    checkpoint_dir='checkpoints',\n    keep_last=3,        # Keep last 3 checkpoints\n    keep_best=5,        # Keep top 5 checkpoints\n    metric_name='val_loss',\n    mode='min'          # Lower is better\n)\n\n# Save checkpoint\nmanager.save(\n    model=model,\n    optimizer=optimizer,\n    epoch=epoch,\n    metrics={'val_loss': 0.5, 'val_acc': 0.95}\n)\n\n# Load latest checkpoint\ncheckpoint = manager.load_latest()\n\n# Load best checkpoint\nbest_checkpoint = manager.load_best()\n</code></pre>"},{"location":"getting-started/concepts/#experiment-tracking","title":"Experiment Tracking","text":"<p>Integrate with popular tracking tools.</p>"},{"location":"getting-started/concepts/#weights-biases","title":"Weights &amp; Biases","text":"<pre><code>from expmate.tracking import WandbTracker\n\ntracker = WandbTracker(\n    project=\"my-project\",\n    name=config.run_id,\n    config=config.to_dict()\n)\n\ntracker.log({'train/loss': loss}, step=epoch)\ntracker.finish()\n</code></pre>"},{"location":"getting-started/concepts/#tensorboard","title":"TensorBoard","text":"<pre><code>from expmate.tracking import TensorBoardTracker\n\ntracker = TensorBoardTracker(log_dir=f'runs/{config.run_id}/tb')\ntracker.log({'loss': loss}, step=epoch)\n</code></pre>"},{"location":"getting-started/concepts/#run-organization","title":"Run Organization","text":"<p>ExpMate automatically organizes experiments:</p> <pre><code>runs/\n\u251c\u2500\u2500 exp_20250123_100000/\n\u2502   \u251c\u2500\u2500 run.yaml          # Config snapshot\n\u2502   \u251c\u2500\u2500 exp.log           # Human-readable log\n\u2502   \u251c\u2500\u2500 events.jsonl      # Machine-readable events\n\u2502   \u251c\u2500\u2500 metrics.csv       # All metrics\n\u2502   \u251c\u2500\u2500 best.json         # Best metric values\n\u2502   \u2514\u2500\u2500 git_info.json     # Git commit info\n\u251c\u2500\u2500 exp_20250123_110000/\n\u2514\u2500\u2500 exp_20250123_120000/\n</code></pre>"},{"location":"getting-started/concepts/#next-steps","title":"Next Steps","text":"<ul> <li>Configuration Management</li> <li>Experiment Logging</li> <li>Checkpoint Management</li> <li>Distributed Training</li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<p>ExpMate requires Python 3.8 or later.</p>"},{"location":"getting-started/installation/#basic-installation","title":"Basic Installation","text":"<p>Install ExpMate using pip:</p> <pre><code>pip install expmate\n</code></pre> <p>This installs the core package with all essential dependencies: - \u2705 Configuration parser - YAML configs with CLI overrides (pyyaml) - \u2705 Experiment logger - Structured logging and metrics tracking (numpy) - \u2705 CLI tools - <code>compare</code>, <code>visualize</code>, <code>sweep</code> commands - \u2705 Visualization - Plot metrics with matplotlib - \u2705 Data analysis - Fast data processing with polars - \u2705 System monitoring - Track CPU/memory usage with psutil</p>"},{"location":"getting-started/installation/#optional-dependencies","title":"Optional Dependencies","text":""},{"location":"getting-started/installation/#experiment-tracking","title":"Experiment Tracking","text":"<p>For integration with Weights &amp; Biases and TensorBoard:</p> <pre><code># Weights &amp; Biases only\npip install expmate[wandb]\n\n# TensorBoard only\npip install expmate[tensorboard]\n\n# Both tracking platforms\npip install expmate[tracking]\n</code></pre>"},{"location":"getting-started/installation/#development-tools","title":"Development Tools","text":"<p>For contributing to ExpMate:</p> <pre><code>pip install expmate[dev]\n</code></pre> <p>This includes: - <code>pytest&gt;=7.0.0</code> - <code>pytest-cov&gt;=4.0.0</code> - <code>black&gt;=23.0.0</code> - <code>ruff&gt;=0.1.0</code> - <code>mypy&gt;=1.0.0</code> - <code>pre-commit&gt;=3.0.0</code></p>"},{"location":"getting-started/installation/#install-from-source","title":"Install from Source","text":"<p>To install from source (for development or latest features):</p> <pre><code>git clone https://github.com/kunheek/expmate.git\ncd expmate\npip install -e .\n</code></pre>"},{"location":"getting-started/installation/#verify-installation","title":"Verify Installation","text":"<p>Verify the installation by checking the version:</p> <pre><code>python -c \"import expmate; print(expmate.__version__)\"\n</code></pre> <p>Or use the CLI:</p> <pre><code>expmate --version\n</code></pre>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<ul> <li>Quick Start Guide</li> <li>Basic Concepts</li> </ul>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":"<p>This guide will walk you through creating your first experiment with ExpMate.</p>"},{"location":"getting-started/quickstart/#step-1-create-a-configuration-file","title":"Step 1: Create a Configuration File","text":"<p>Create a YAML configuration file <code>config.yaml</code>:</p> <pre><code># Run identification\nrun_id: \"exp_${now:%Y%m%d_%H%M%S}\"\nseed: 42\n\n# Model architecture\nmodel:\n  input_dim: 128\n  hidden_dim: 256\n  output_dim: 10\n\n# Training configuration\ntraining:\n  epochs: 10\n  lr: 0.001\n  batch_size: 32\n</code></pre>"},{"location":"getting-started/quickstart/#step-2-write-your-training-script","title":"Step 2: Write Your Training Script","text":"<p>Create <code>train.py</code>:</p> <pre><code>import torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, TensorDataset\n\nfrom expmate import ExperimentLogger, parse_config, set_seed\n\ndef main():\n    # Parse config from command line\n    config = parse_config()\n\n    # Set random seed for reproducibility\n    set_seed(config.seed)\n\n    # Create experiment logger\n    logger = ExperimentLogger(run_dir=f\"runs/{config.run_id}\")\n    logger.info(f\"Starting experiment: {config.run_id}\")\n    logger.info(f\"Config: {config.to_dict()}\")\n\n    # Create model\n    model = nn.Sequential(\n        nn.Linear(config.model.input_dim, config.model.hidden_dim),\n        nn.ReLU(),\n        nn.Linear(config.model.hidden_dim, config.model.output_dim)\n    )\n\n    # Create dummy dataset\n    dataset = TensorDataset(\n        torch.randn(1000, config.model.input_dim),\n        torch.randint(0, config.model.output_dim, (1000,))\n    )\n    dataloader = DataLoader(dataset, batch_size=config.training.batch_size)\n\n    # Setup training\n    optimizer = torch.optim.Adam(model.parameters(), lr=config.training.lr)\n    criterion = nn.CrossEntropyLoss()\n\n    # Training loop\n    for epoch in range(config.training.epochs):\n        total_loss = 0\n\n        for batch_x, batch_y in dataloader:\n            output = model(batch_x)\n            loss = criterion(output, batch_y)\n\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n\n            total_loss += loss.item()\n\n        avg_loss = total_loss / len(dataloader)\n\n        # Log metrics\n        logger.log_metric(step=epoch, split='train', name='loss', value=avg_loss)\n        logger.info(f\"Epoch {epoch}/{config.training.epochs}: loss={avg_loss:.4f}\")\n\n    logger.info(\"Training complete!\")\n    logger.info(f\"Logs saved to: {logger.run_dir}\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"getting-started/quickstart/#step-3-run-your-experiment","title":"Step 3: Run Your Experiment","text":"<p>Run with default config:</p> <pre><code>python train.py config.yaml\n</code></pre> <p>Run with parameter overrides:</p> <pre><code>python train.py config.yaml training.lr=0.01 training.epochs=20\n</code></pre> <p>Add new parameters:</p> <pre><code>python train.py config.yaml +optimizer.weight_decay=0.0001\n</code></pre>"},{"location":"getting-started/quickstart/#step-4-check-your-results","title":"Step 4: Check Your Results","text":"<p>ExpMate automatically creates a run directory with the following structure:</p> <pre><code>runs/\n\u2514\u2500\u2500 exp_20250123_143022/\n    \u251c\u2500\u2500 run.yaml              # Full config used for this run\n    \u251c\u2500\u2500 exp.log               # Human-readable logs\n    \u251c\u2500\u2500 events.jsonl          # Machine-readable event logs\n    \u251c\u2500\u2500 metrics.csv           # All logged metrics\n    \u2514\u2500\u2500 best.json             # Best metric values\n</code></pre>"},{"location":"getting-started/quickstart/#step-5-compare-experiments","title":"Step 5: Compare Experiments","text":"<p>Run multiple experiments with different hyperparameters:</p> <pre><code>python train.py config.yaml training.lr=0.001\npython train.py config.yaml training.lr=0.01\npython train.py config.yaml training.lr=0.1\n</code></pre> <p>Compare them using the CLI:</p> <pre><code>expmate compare runs/exp_*\n</code></pre> <p>Visualize training curves:</p> <pre><code>expmate viz runs/exp_* --metrics loss\n</code></pre>"},{"location":"getting-started/quickstart/#whats-next","title":"What's Next?","text":"<ul> <li>Learn about Basic Concepts</li> <li>Explore Configuration Management</li> <li>Check out Complete Examples</li> </ul>"},{"location":"guide/checkpoints/","title":"Checkpoint Management","text":"<p>(Documentation in progress - will be added in next update)</p> <p>For now, see the API Reference and examples.</p>"},{"location":"guide/cli/","title":"CLI Tools","text":"<p>ExpMate provides command-line tools for experiment management and analysis.</p>"},{"location":"guide/cli/#installation","title":"Installation","text":"<p>CLI tools are included with the base installation:</p> <pre><code>pip install expmate\n</code></pre> <p>Verify installation:</p> <pre><code>expmate --help\n</code></pre>"},{"location":"guide/cli/#commands","title":"Commands","text":""},{"location":"guide/cli/#compare-runs","title":"Compare Runs","text":"<p>Compare multiple experiment runs and their metrics.</p>"},{"location":"guide/cli/#basic-usage","title":"Basic Usage","text":"<pre><code># Compare all runs in a directory\nexpmate compare runs/exp_*\n\n# Compare specific runs\nexpmate compare runs/exp1 runs/exp2 runs/exp3\n</code></pre>"},{"location":"guide/cli/#options","title":"Options","text":"<pre><code># Compare specific metrics\nexpmate compare runs/exp_* --metrics loss accuracy\n\n# Show configuration differences\nexpmate compare runs/exp_* --show-config\n\n# Show git information\nexpmate compare runs/exp_* --show-git\n\n# Export to CSV\nexpmate compare runs/exp_* --output results.csv\n\n# Pretty print table\nexpmate compare runs/exp_* --format table\n</code></pre>"},{"location":"guide/cli/#example-output","title":"Example Output","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 run_id               \u2502 best_loss  \u2502 best_acc    \u2502 final_loss   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 exp_20250123_100000  \u2502 0.234      \u2502 0.956       \u2502 0.245        \u2502\n\u2502 exp_20250123_110000  \u2502 0.189      \u2502 0.967       \u2502 0.201        \u2502\n\u2502 exp_20250123_120000  \u2502 0.156      \u2502 0.978       \u2502 0.167        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"guide/cli/#visualize-metrics","title":"Visualize Metrics","text":"<p>Plot training curves and compare experiments.</p>"},{"location":"guide/cli/#basic-usage_1","title":"Basic Usage","text":"<pre><code># Plot metrics for a single run\nexpmate viz runs/exp1\n\n# Plot specific metrics\nexpmate viz runs/exp1 --metrics loss accuracy\n\n# Compare multiple runs\nexpmate viz runs/exp1 runs/exp2 runs/exp3 --metrics loss\n</code></pre>"},{"location":"guide/cli/#options_1","title":"Options","text":"<pre><code># Specify output file\nexpmate viz runs/exp1 --output metrics.png\n\n# Choose plot style\nexpmate viz runs/exp1 --style seaborn\nexpmate viz runs/exp1 --style ggplot\n\n# Filter by split\nexpmate viz runs/exp1 --split train\nexpmate viz runs/exp1 --split val\n\n# Smooth curves\nexpmate viz runs/exp1 --smooth 0.9\n\n# Set axis limits\nexpmate viz runs/exp1 --ylim 0 1\n</code></pre>"},{"location":"guide/cli/#example","title":"Example","text":"<pre><code># Create comparison plot\nexpmate viz \\\n    runs/exp_lr_0.001 \\\n    runs/exp_lr_0.01 \\\n    runs/exp_lr_0.1 \\\n    --metrics loss accuracy \\\n    --style seaborn \\\n    --output learning_rate_comparison.png\n</code></pre>"},{"location":"guide/cli/#hyperparameter-sweeps","title":"Hyperparameter Sweeps","text":"<p>Run systematic hyperparameter searches with grid search.</p>"},{"location":"guide/cli/#basic-grid-search","title":"Basic Grid Search","text":"<pre><code># Basic sweep - creates all combinations\nexpmate sweep \"python train.py {config}\" \\\n  --config config.yaml \\\n  --sweep \"training.lr=[0.001,0.01,0.1]\" \\\n          \"model.hidden_dim=[128,256,512]\"\n</code></pre> <p>This creates 3 \u00d7 3 = 9 runs with all combinations of the parameters.</p> <p>The <code>{config}</code> placeholder is replaced with the path to each generated config file.</p>"},{"location":"guide/cli/#with-distributed-training","title":"With Distributed Training","text":"<p>Use <code>torchrun</code> in the command template:</p> <pre><code>expmate sweep \"torchrun --nproc_per_node=4 train.py {config}\" \\\n  --config config.yaml \\\n  --sweep \"training.lr=[0.001,0.01,0.1]\"\n</code></pre>"},{"location":"guide/cli/#sweep-options","title":"Sweep Options","text":"<pre><code># Custom sweep name\nexpmate sweep \"python train.py {config}\" \\\n  --config config.yaml \\\n  --name my_lr_sweep \\\n  --sweep \"training.lr=[0.001,0.01,0.1]\"\n\n# Custom runs directory\nexpmate sweep \"python train.py {config}\" \\\n  --config config.yaml \\\n  --runs-dir sweeps/experiments \\\n  --sweep \"training.lr=[0.001,0.01,0.1]\"\n\n# Dry run (preview commands without running)\nexpmate sweep \"python train.py {config}\" \\\n  --config config.yaml \\\n  --sweep \"training.lr=[0.001,0.01,0.1]\" \\\n  --dry-run\n</code></pre>"},{"location":"guide/cli/#multiple-parameters","title":"Multiple Parameters","text":"<p>Sweep over multiple parameters:</p> <pre><code>expmate sweep \"python train.py {config}\" \\\n  --config config.yaml \\\n  --sweep \\\n    \"training.lr=[0.0001,0.001,0.01]\" \\\n    \"training.weight_decay=[0,0.0001,0.001]\" \\\n    \"model.dropout=[0.1,0.2,0.3]\"\n</code></pre> <p>This creates 3 \u00d7 3 \u00d7 3 = 27 runs.</p>"},{"location":"guide/cli/#parameter-value-types","title":"Parameter Value Types","text":"<p>The sweep command automatically detects types:</p> <pre><code># Floats\n--sweep \"training.lr=[0.001,0.01,0.1]\"\n\n# Integers\n--sweep \"model.hidden_dim=[128,256,512]\"\n\n# Strings\n--sweep \"model.activation=[relu,gelu,silu]\"\n\n# Booleans\n--sweep \"training.use_amp=[true,false]\"\n\n# Mixed types\n--sweep \"training.batch_size=[16,32,64]\" \\\n        \"optimizer.name=[adam,sgd,adamw]\"\n</code></pre>"},{"location":"guide/cli/#example-workflow","title":"Example Workflow","text":"<pre><code># 1. Run hyperparameter sweep\nexpmate sweep \"python train.py {config}\" \\\n  --config config.yaml \\\n  --name lr_wd_sweep \\\n  --runs-dir sweeps/lr_wd \\\n  --sweep \\\n    \"training.lr=[0.0001,0.001,0.01]\" \\\n    \"training.weight_decay=[0,0.0001,0.001]\"\n\n# 2. Compare results\nexpmate compare sweeps/lr_wd/exp_* --output sweep_results.csv\n\n# 3. Visualize best runs\nexpmate viz \\\n    sweeps/lr_wd/exp_000 \\\n    sweeps/lr_wd/exp_004 \\\n    sweeps/lr_wd/exp_008 \\\n    --metrics val_loss val_accuracy\n</code></pre>"},{"location":"guide/cli/#output-structure","title":"Output Structure","text":"<p>Each sweep creates a directory structure:</p> <pre><code>sweeps/lr_wd_sweep_20250123_143022/\n\u251c\u2500\u2500 sweep_info.json           # Sweep configuration and metadata\n\u251c\u2500\u2500 exp_000/\n\u2502   \u251c\u2500\u2500 config.yaml           # Generated config for this run\n\u2502   \u251c\u2500\u2500 run.yaml              # Full config after execution\n\u2502   \u251c\u2500\u2500 exp.log               # Logs\n\u2502   \u2514\u2500\u2500 metrics.csv           # Metrics\n\u251c\u2500\u2500 exp_001/\n\u2502   \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 exp_008/\n    \u2514\u2500\u2500 ...\n</code></pre>"},{"location":"guide/cli/#python-api","title":"Python API","text":"<p>You can also use CLI functionality from Python:</p>"},{"location":"guide/cli/#generate-sweep-configurations","title":"Generate Sweep Configurations","text":"<pre><code>from expmate.cli.sweep import generate_sweep_configs\n\n# Generate all configurations for a grid search\nconfigs = generate_sweep_configs(\n    base_config={'model': {'depth': 18}, 'training': {'epochs': 100}},\n    sweep_params={\n        'training.lr': [0.001, 0.01, 0.1],\n        'model.hidden_dim': [128, 256, 512]\n    }\n)\n\n# Returns list of 9 configurations with all combinations\nfor i, config in enumerate(configs):\n    print(f\"Config {i}: lr={config['training']['lr']}, \"\n          f\"hidden_dim={config['model']['hidden_dim']}\")\n</code></pre>"},{"location":"guide/cli/#run-sweep","title":"Run Sweep","text":"<pre><code>from expmate.cli.sweep import run_sweep\n\n# Run the sweep\nrun_sweep(\n    command_template=\"python train.py {config}\",\n    sweep_params={\n        'training.lr': [0.001, 0.01, 0.1],\n        'model.hidden_dim': [128, 256, 512]\n    },\n    base_config_file='config.yaml',\n    sweep_name='my_sweep',\n    runs_dir='sweeps',\n    dry_run=False  # Set True to preview without running\n)\n</code></pre>"},{"location":"guide/cli/#advanced-usage","title":"Advanced Usage","text":""},{"location":"guide/cli/#custom-analysis-scripts","title":"Custom Analysis Scripts","text":"<p>Combine CLI tools with custom analysis:</p> <pre><code>import pandas as pd\nfrom expmate.cli.compare import load_run_info\n\n# Load all runs\nruns = [load_run_info(f'runs/{d}') for d in os.listdir('runs')]\n\n# Filter runs by config\nhigh_lr_runs = [r for r in runs if r['config']['training']['lr'] &gt; 0.01]\n\n# Analyze\ndf = pd.DataFrame([{\n    'run_id': r['run_id'],\n    'lr': r['config']['training']['lr'],\n    'best_loss': r['best_metrics']['loss']['value']\n} for r in high_lr_runs])\n\nprint(df.sort_values('best_loss'))\n</code></pre>"},{"location":"guide/cli/#integration-with-other-tools","title":"Integration with Other Tools","text":"<pre><code># Export to pandas for analysis\nexpmate compare runs/exp_* --output results.csv\npython analyze.py results.csv\n\n# Generate plots for reports\nexpmate viz runs/exp_* --output figure1.png\nexpmate viz runs/exp_* --metrics accuracy --output figure2.png\n\n# Create summary report\nexpmate compare runs/exp_* &gt; results.txt\ncat results.txt\n</code></pre>"},{"location":"guide/cli/#tips","title":"Tips","text":"<ol> <li>Use glob patterns to match multiple runs: <code>runs/exp_*</code></li> <li>Export to CSV for custom analysis: <code>--output results.csv</code></li> <li>Run sweeps in parallel to save time: <code>--parallel 4</code></li> <li>Use dry run to verify commands: <code>--dry-run</code></li> <li>Combine tools for comprehensive analysis</li> </ol>"},{"location":"guide/cli/#see-also","title":"See Also","text":"<ul> <li>Configuration Management</li> <li>Experiment Logging</li> <li>Examples: Hyperparameter Sweeps</li> </ul>"},{"location":"guide/configuration/","title":"Configuration Management","text":"<p>ExpMate provides a powerful typed configuration system using Python dataclasses with full IDE support.</p>"},{"location":"guide/configuration/#quick-start","title":"Quick Start","text":""},{"location":"guide/configuration/#1-define-your-config","title":"1. Define Your Config","text":"<p>Use dataclasses with type hints for IDE autocomplete and validation:</p> <pre><code>from dataclasses import dataclass, field\nfrom expmate import Config\n\n@dataclass\nclass ModelConfig(Config):\n    \"\"\"Model configuration\"\"\"\n    hidden_dim: int = 256\n    dropout: float = 0.1\n    layers: list[int] = field(default_factory=lambda: [128, 256, 512])\n\n@dataclass\nclass TrainingConfig(Config):\n    \"\"\"Training configuration\"\"\"\n    lr: float = 0.001\n    epochs: int = 100\n    batch_size: int = 32\n\n@dataclass\nclass ExperimentConfig(Config):\n    \"\"\"Main experiment configuration\"\"\"\n    run_id: str = \"exp_${now:%Y%m%d_%H%M%S}\"\n    seed: int = 42\n    model: ModelConfig = field(default_factory=ModelConfig)\n    training: TrainingConfig = field(default_factory=TrainingConfig)\n</code></pre>"},{"location":"guide/configuration/#2-create-yaml-config","title":"2. Create YAML Config","text":"<pre><code># config.yaml\nseed: 42\nmodel:\n  hidden_dim: 256\n  dropout: 0.1\ntraining:\n  lr: 0.001\n  epochs: 100\n  batch_size: 32\n</code></pre>"},{"location":"guide/configuration/#3-load-config-with-overrides","title":"3. Load Config with Overrides","text":"<pre><code>import argparse\n\n# Parse arguments\nparser = argparse.ArgumentParser()\nparser.add_argument(\"config\", help=\"Path to config file\")\nargs, overrides = parser.parse_known_args()\n\n# Load config with CLI overrides\nconfig = ExperimentConfig.from_file(args.config, overrides=overrides)\n</code></pre>"},{"location":"guide/configuration/#4-access-config-values","title":"4. Access Config Values","text":"<pre><code># Typed access with IDE autocomplete\nprint(config.model.hidden_dim)  # 256\nprint(config.training.lr)       # 0.001\n\n# Dictionary-style access also works\nprint(config['model']['hidden_dim'])  # 256\n</code></pre>"},{"location":"guide/configuration/#loading-configurations","title":"Loading Configurations","text":""},{"location":"guide/configuration/#from-yaml-file","title":"From YAML File","text":"<pre><code># Load with defaults\nconfig = ExperimentConfig.from_file('config.yaml')\n\n# Load with CLI overrides\nconfig = ExperimentConfig.from_file('config.yaml', overrides=overrides)\n</code></pre>"},{"location":"guide/configuration/#from-dictionary","title":"From Dictionary","text":"<pre><code>config_dict = {\n    'seed': 42,\n    'model': {'hidden_dim': 256},\n    'training': {'lr': 0.001}\n}\nconfig = ExperimentConfig.from_dict(config_dict)\n</code></pre>"},{"location":"guide/configuration/#with-typed-validation","title":"With Typed Validation","text":"<p>The dataclass automatically validates types:</p> <pre><code>@dataclass\nclass Config(Config):\n    lr: float = 0.001\n    epochs: int = 100\n\n# This will validate types\nconfig = Config.from_dict({'lr': 0.01, 'epochs': 50})  # \u2713\nconfig = Config.from_dict({'lr': 'invalid'})  # \u2717 Type error\n</code></pre>"},{"location":"guide/configuration/#command-line-integration","title":"Command-Line Integration","text":""},{"location":"guide/configuration/#recommended-pattern-with-argparse","title":"Recommended Pattern with argparse","text":"<pre><code>import argparse\nfrom dataclasses import dataclass, field\nfrom expmate import Config\n\n@dataclass\nclass ExperimentConfig(Config):\n    run_id: str = \"exp_${now:%Y%m%d_%H%M%S}\"\n    seed: int = 42\n    # ... other fields\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Train model\")\n    parser.add_argument(\"config\", help=\"Path to config file\")\n    parser.add_argument(\"--debug\", action=\"store_true\", help=\"Debug mode\")\n\n    # parse_known_args returns (args, unknown_args)\n    args, overrides = parser.parse_known_args()\n\n    # Load config with overrides\n    config = ExperimentConfig.from_file(args.config, overrides=overrides)\n\n    # Use config\n    print(f\"Running experiment: {config.run_id}\")\n    print(f\"Model: {config.model.hidden_dim} hidden dims\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>Usage:</p> <pre><code># Basic usage\npython train.py config.yaml\n\n# With overrides\npython train.py config.yaml +training.lr=0.01 +model.hidden_dim=512\n\n# With argparse flags\npython train.py config.yaml --debug +training.epochs=50\n</code></pre>"},{"location":"guide/configuration/#override-syntax","title":"Override Syntax","text":""},{"location":"guide/configuration/#basic-override","title":"Basic Override","text":"<p>Override existing values:</p> <pre><code>python train.py config.yaml +training.lr=0.01\npython train.py config.yaml +model.hidden_dim=512\n</code></pre>"},{"location":"guide/configuration/#nested-values","title":"Nested Values","text":"<p>Use dot notation for deeply nested values:</p> <pre><code>python train.py config.yaml +model.hidden_dim=512\npython train.py config.yaml +training.optimizer.lr=0.01\npython train.py config.yaml +data.augmentation.strength=0.5\n</code></pre>"},{"location":"guide/configuration/#adding-new-keys","title":"Adding New Keys","text":"<p>Add new keys that don't exist in the config:</p> <pre><code># Add new key\npython train.py config.yaml +optimizer.weight_decay:float=0.0001\n\n# Add nested new key\npython train.py config.yaml +training.warmup_steps:int=1000\n</code></pre> <p>Without <code>+</code>, overriding a non-existent key will raise an error for safety.</p>"},{"location":"guide/configuration/#type-hints","title":"Type Hints","text":"<p>Use type hints to explicitly specify the type:</p> <pre><code># Force float type (useful for scientific notation)\npython train.py config.yaml +training.lr:float=1e-3\n\n# Force int type\npython train.py config.yaml +model.num_layers:int=12\n\n# Force string type\npython train.py config.yaml +model.name:str=resnet50\n\n# Force boolean type\npython train.py config.yaml +training.use_amp:bool=true\n</code></pre>"},{"location":"guide/configuration/#lists","title":"Lists","text":"<p>ExpMate supports multiple ways to specify lists:</p>"},{"location":"guide/configuration/#space-separated-lists-recommended","title":"Space-Separated Lists (Recommended)","text":"<p>The most natural way - just separate values with spaces:</p> <pre><code># Auto-detect type (integers)\npython train.py config.yaml +training.gpu_ids:int 0 1 2 3\n\n# Auto-detect type (floats)\npython train.py config.yaml +model.dropout_rates:float 0.1 0.2 0.3\n\n# Strings\npython train.py config.yaml +data.splits:str train val test\n\n# Mixed with other overrides\npython train.py config.yaml +model.layers:int 128 256 512 training.lr=0.001\n</code></pre>"},{"location":"guide/configuration/#json-lists","title":"JSON Lists","text":"<p>Use JSON syntax for lists:</p> <pre><code># Integers\npython train.py config.yaml +model.layers=[128,256,512]\n\n# Floats  \npython train.py config.yaml +model.dropout=[0.1,0.2,0.3]\n\n# Strings (needs quotes)\npython train.py config.yaml '+data.splits=[\"train\",\"val\",\"test\"]'\n</code></pre> <p>Note: JSON lists with strings need shell quotes to prevent parsing issues.</p>"},{"location":"guide/configuration/#dictionaries","title":"Dictionaries","text":"<p>Use JSON syntax for dictionaries:</p> <pre><code>python train.py config.yaml '+optimizer={\"name\":\"adam\",\"lr\":0.001,\"betas\":[0.9,0.999]}'\n</code></pre>"},{"location":"guide/configuration/#boolean-values","title":"Boolean Values","text":"<p>Multiple formats accepted:</p> <pre><code># Standard boolean\npython train.py config.yaml +training.use_amp=true\npython train.py config.yaml +training.use_amp=false\n\n# Also accepts\npython train.py config.yaml +training.use_amp=True\npython train.py config.yaml +training.use_amp=False\npython train.py config.yaml +training.use_amp=1\npython train.py config.yaml +training.use_amp=0\n</code></pre>"},{"location":"guide/configuration/#examples","title":"Examples","text":"<pre><code># Single override\npython train.py config.yaml +training.lr=0.01\n\n# Multiple overrides\npython train.py config.yaml +training.lr=0.01 +model.hidden_dim=512 +seed=123\n\n# List as space-separated values\npython train.py config.yaml +model.layers:int 128 256 512\n\n# List as JSON\npython train.py config.yaml +model.layers=[128,256,512]\n\n# Type hints\npython train.py config.yaml +training.lr:float=1e-3 +model.dropout:float=0.1\n\n# Complex combination\npython train.py config.yaml \\\n    +model.layers:int 128 256 512 \\\n    training.lr=0.001 \\\n    +training.warmup_steps:int=1000 \\\n    +seed=42\n</code></pre>"},{"location":"guide/configuration/#variable-interpolation","title":"Variable Interpolation","text":"<p>ExpMate supports dynamic variable interpolation in config files.</p>"},{"location":"guide/configuration/#timestamp-variables","title":"Timestamp Variables","text":"<p>Use <code>${now:format}</code> for timestamps:</p> <pre><code>run_id: \"exp_${now:%Y%m%d_%H%M%S}\"  # exp_20250128_143022\nlog_dir: \"logs/${now:%Y%m%d}\"        # logs/20250128\ncheckpoint: \"ckpt_${now:%H%M%S}.pt\"  # ckpt_143022.pt\n</code></pre> <p>Format codes follow Python's strftime: - <code>%Y</code>: 4-digit year (2025) - <code>%y</code>: 2-digit year (25) - <code>%m</code>: Month (01-12) - <code>%d</code>: Day (01-31) - <code>%H</code>: Hour (00-23) - <code>%M</code>: Minute (00-59) - <code>%S</code>: Second (00-59)</p>"},{"location":"guide/configuration/#environment-variables","title":"Environment Variables","text":"<p>Reference environment variables with <code>${VAR_NAME}</code>:</p> <pre><code>data_dir: \"${DATA_ROOT}/train\"\ncache_dir: \"${HOME}/.cache/expmate\"\noutput_dir: \"${SCRATCH}/experiments\"\napi_key: \"${WANDB_API_KEY}\"\n</code></pre>"},{"location":"guide/configuration/#config-references","title":"Config References","text":"<p>Reference other config values:</p> <pre><code>model:\n  hidden_dim: 256\n\ntraining:\n  batch_size: 32\n\n# Reference other values\nmodel_name: \"model_h${model.hidden_dim}\"      # model_h256\noutput_dir: \"outputs/${run_id}\"\ncache_path: \"${data.root}/.cache\"\n</code></pre>"},{"location":"guide/configuration/#hostname-variables","title":"Hostname Variables","text":"<p>Use <code>${hostname}</code> for machine-specific configs:</p> <pre><code>run_id: \"exp_${hostname}_${now:%Y%m%d_%H%M%S}\"\ncache_dir: \"/tmp/${hostname}/cache\"\n</code></pre>"},{"location":"guide/configuration/#combined-example","title":"Combined Example","text":"<pre><code># Dynamic run identification\nrun_id: \"exp_${now:%Y%m%d_%H%M%S}\"\nhostname: \"${hostname}\"\n\n# Paths with interpolation\ndata_root: \"${DATA_ROOT}/datasets\"\noutput_dir: \"outputs/${run_id}\"\ncheckpoint_dir: \"${output_dir}/checkpoints\"\nlog_file: \"${output_dir}/${run_id}.log\"\n\nmodel:\n  name: \"resnet_${model.depth}\"\n  depth: 50\n</code></pre>"},{"location":"guide/configuration/#config-access-patterns","title":"Config Access Patterns","text":""},{"location":"guide/configuration/#attribute-access-recommended","title":"Attribute Access (Recommended)","text":"<p>Clean, type-safe access with IDE support:</p> <pre><code>@dataclass\nclass ModelConfig(Config):\n    hidden_dim: int = 256\n    dropout: float = 0.1\n\n@dataclass\nclass ExperimentConfig(Config):\n    model: ModelConfig = field(default_factory=ModelConfig)\n\nconfig = ExperimentConfig.from_file('config.yaml')\n\n# Attribute access with autocomplete\nprint(config.model.hidden_dim)  # 256\nprint(config.model.dropout)     # 0.1\n\n# Type-safe: your IDE knows the types!\nconfig.model.hidden_dim = 512   # \u2713\nconfig.model.hidden_dim = \"invalid\"  # \u2717 Type error\n</code></pre>"},{"location":"guide/configuration/#dictionary-access","title":"Dictionary Access","text":"<p>Also supports dictionary-style access:</p> <pre><code># Dictionary-style\nprint(config['model']['hidden_dim'])  # 256\n\n# Mixed access\nprint(config.model['dropout'])        # 0.1\nprint(config['model'].dropout)        # 0.1\n</code></pre>"},{"location":"guide/configuration/#iteration","title":"Iteration","text":"<pre><code># Iterate over config keys\nfor key in config:\n    print(f\"{key}: {config[key]}\")\n\n# Check if key exists\nif 'model' in config:\n    print(\"Model config found\")\n</code></pre>"},{"location":"guide/configuration/#conversion-methods","title":"Conversion Methods","text":"<pre><code># Convert to dictionary\nconfig_dict = config.to_dict()\n# {'model': {'hidden_dim': 256, 'dropout': 0.1}, ...}\n\n# Convert to flat dictionary\nflat_dict = config.to_flat_dict()\n# {'model.hidden_dim': 256, 'model.dropout': 0.1, ...}\n\n# Convert to YAML string\nyaml_str = config.to_yaml()\n\n# Save to file\nconfig.save('output.yaml')\n</code></pre>"},{"location":"guide/configuration/#saving-configurations","title":"Saving Configurations","text":""},{"location":"guide/configuration/#save-to-yaml","title":"Save to YAML","text":"<pre><code># Save current config\nconfig.save('experiment.yaml')\n\n# Save to specific directory\nconfig.save('runs/exp_001/config.yaml')\n</code></pre>"},{"location":"guide/configuration/#automatic-saving-with-logger","title":"Automatic Saving with Logger","text":"<p>ExpMate automatically saves configs when using ExperimentLogger:</p> <pre><code>from expmate import ExperimentLogger\n\nlogger = ExperimentLogger(run_dir=f\"runs/{config.run_id}\")\n# Automatically saves config to runs/{run_id}/run.yaml\n</code></pre> <p>The saved config includes all applied overrides, making experiments fully reproducible.</p>"},{"location":"guide/configuration/#advanced-patterns","title":"Advanced Patterns","text":""},{"location":"guide/configuration/#nested-dataclasses","title":"Nested Dataclasses","text":"<p>Build complex hierarchical configs:</p> <pre><code>from dataclasses import dataclass, field\nfrom expmate import Config\n\n@dataclass\nclass OptimizerConfig(Config):\n    name: str = \"adam\"\n    lr: float = 0.001\n    betas: tuple[float, float] = (0.9, 0.999)\n    weight_decay: float = 0.0\n\n@dataclass\nclass SchedulerConfig(Config):\n    type: str = \"cosine\"\n    warmup_epochs: int = 10\n    min_lr: float = 1e-6\n\n@dataclass\nclass TrainingConfig(Config):\n    epochs: int = 100\n    batch_size: int = 32\n    optimizer: OptimizerConfig = field(default_factory=OptimizerConfig)\n    scheduler: SchedulerConfig = field(default_factory=SchedulerConfig)\n\n@dataclass\nclass ExperimentConfig(Config):\n    run_id: str = \"exp_${now:%Y%m%d_%H%M%S}\"\n    training: TrainingConfig = field(default_factory=TrainingConfig)\n</code></pre> <p>YAML:</p> <pre><code>training:\n  epochs: 100\n  batch_size: 32\n  optimizer:\n    name: adamw\n    lr: 0.001\n    weight_decay: 0.01\n  scheduler:\n    type: cosine\n    warmup_epochs: 10\n</code></pre> <p>Access:</p> <pre><code>config = ExperimentConfig.from_file('config.yaml')\nprint(config.training.optimizer.lr)           # 0.001\nprint(config.training.scheduler.warmup_epochs) # 10\n</code></pre>"},{"location":"guide/configuration/#optional-fields","title":"Optional Fields","text":"<p>Use <code>Optional[]</code> for nullable fields:</p> <pre><code>from typing import Optional\n\n@dataclass\nclass ModelConfig(Config):\n    name: str = \"resnet50\"\n    pretrained_path: Optional[str] = None  # Can be None or str\n    checkpoint: Optional[str] = None\n\n# Usage\nconfig = ModelConfig.from_dict({\n    'name': 'resnet50',\n    'checkpoint': 'model.pt'  # pretrained_path stays None\n})\n</code></pre>"},{"location":"guide/configuration/#default-factories","title":"Default Factories","text":"<p>Use <code>field(default_factory=...)</code> for mutable defaults:</p> <pre><code>from typing import List\n\n@dataclass\nclass Config(Config):\n    # \u2713 Correct: use default_factory for lists/dicts\n    gpu_ids: List[int] = field(default_factory=lambda: [0, 1, 2, 3])\n    layers: List[int] = field(default_factory=lambda: [128, 256, 512])\n    metadata: dict = field(default_factory=dict)\n\n    # \u2717 Wrong: don't use mutable defaults directly\n    # gpu_ids: List[int] = [0, 1, 2, 3]  # All instances share same list!\n</code></pre>"},{"location":"guide/configuration/#config-validation","title":"Config Validation","text":"<p>Add validation in <code>__post_init__</code>:</p> <pre><code>@dataclass\nclass TrainingConfig(Config):\n    lr: float = 0.001\n    epochs: int = 100\n    batch_size: int = 32\n\n    def __post_init__(self):\n        # Validation\n        if self.lr &lt;= 0:\n            raise ValueError(f\"lr must be positive, got {self.lr}\")\n        if self.epochs &lt;= 0:\n            raise ValueError(f\"epochs must be positive, got {self.epochs}\")\n        if self.batch_size &lt;= 0:\n            raise ValueError(f\"batch_size must be positive, got {self.batch_size}\")\n\n        # Derived values\n        self.total_steps = self.epochs * 1000  # Assume 1000 steps/epoch\n</code></pre>"},{"location":"guide/configuration/#multiple-config-sources","title":"Multiple Config Sources","text":"<p>Merge configs from multiple files:</p> <pre><code># Load base config\nconfig = ExperimentConfig.from_file('base.yaml')\n\n# Override with experiment-specific config\nexp_config = ExperimentConfig.from_file('experiment.yaml')\nconfig.update(exp_config.to_dict())\n\n# Apply CLI overrides\nconfig = ExperimentConfig.from_dict(config.to_dict(), overrides=overrides)\n</code></pre> <p>Or use YAML anchors/references:</p> <pre><code># base.yaml\ndefaults: &amp;defaults\n  seed: 42\n  device: cuda\n\nexperiment1:\n  &lt;&lt;: *defaults\n  lr: 0.001\n\nexperiment2:\n  &lt;&lt;: *defaults\n  lr: 0.01\n</code></pre>"},{"location":"guide/configuration/#best-practices","title":"Best Practices","text":""},{"location":"guide/configuration/#1-use-typed-dataclasses","title":"1. Use Typed Dataclasses","text":"<p>Always use typed dataclasses for IDE support and validation:</p> <pre><code># \u2713 Good: Typed with IDE autocomplete\n@dataclass\nclass ModelConfig(Config):\n    hidden_dim: int = 256\n    dropout: float = 0.1\n\n# \u2717 Avoid: Plain dictionary (no type checking)\nconfig = Config({'hidden_dim': 256, 'dropout': 0.1})\n</code></pre>"},{"location":"guide/configuration/#2-hierarchical-organization","title":"2. Hierarchical Organization","text":"<p>Organize configs by component:</p> <pre><code>@dataclass\nclass DataConfig(Config):\n    \"\"\"Data loading configuration\"\"\"\n    batch_size: int = 32\n    num_workers: int = 4\n\n@dataclass\nclass ModelConfig(Config):\n    \"\"\"Model architecture configuration\"\"\"\n    hidden_dim: int = 256\n    num_layers: int = 12\n\n@dataclass\nclass TrainingConfig(Config):\n    \"\"\"Training configuration\"\"\"\n    lr: float = 0.001\n    epochs: int = 100\n\n@dataclass\nclass ExperimentConfig(Config):\n    \"\"\"Main experiment configuration\"\"\"\n    data: DataConfig = field(default_factory=DataConfig)\n    model: ModelConfig = field(default_factory=ModelConfig)\n    training: TrainingConfig = field(default_factory=TrainingConfig)\n</code></pre>"},{"location":"guide/configuration/#3-document-your-config","title":"3. Document Your Config","text":"<p>Use docstrings and comments:</p> <pre><code>@dataclass\nclass TrainingConfig(Config):\n    \"\"\"Training hyperparameters\"\"\"\n\n    lr: float = 0.001\n    \"\"\"Learning rate (default: 0.001)\"\"\"\n\n    epochs: int = 100\n    \"\"\"Number of training epochs\"\"\"\n\n    warmup_steps: int = 1000\n    \"\"\"Number of warmup steps for learning rate scheduler\"\"\"\n</code></pre>"},{"location":"guide/configuration/#4-sensible-defaults","title":"4. Sensible Defaults","text":"<p>Provide working defaults for all parameters:</p> <pre><code>@dataclass\nclass Config(Config):\n    # \u2713 Good: sensible defaults\n    seed: int = 42\n    device: str = \"cuda\"\n    lr: float = 0.001\n\n    # \u2717 Avoid: requiring user to set everything\n    # seed: int  # No default!\n</code></pre>"},{"location":"guide/configuration/#5-use-variable-interpolation","title":"5. Use Variable Interpolation","text":"<p>Avoid repetition with interpolation:</p> <pre><code># \u2713 Good: DRY principle\nrun_id: \"exp_${now:%Y%m%d_%H%M%S}\"\noutput_dir: \"outputs/${run_id}\"\ncheckpoint_dir: \"${output_dir}/checkpoints\"\nlog_dir: \"${output_dir}/logs\"\n\n# \u2717 Avoid: repetition\nrun_id: \"exp_20250128_143022\"\noutput_dir: \"outputs/exp_20250128_143022\"\ncheckpoint_dir: \"outputs/exp_20250128_143022/checkpoints\"\n</code></pre>"},{"location":"guide/configuration/#6-validation","title":"6. Validation","text":"<p>Add validation for critical parameters:</p> <pre><code>@dataclass\nclass TrainingConfig(Config):\n    lr: float = 0.001\n    batch_size: int = 32\n\n    def __post_init__(self):\n        if self.lr &lt;= 0:\n            raise ValueError(f\"Learning rate must be positive, got {self.lr}\")\n        if self.batch_size &lt;= 0:\n            raise ValueError(f\"Batch size must be positive, got {self.batch_size}\")\n</code></pre>"},{"location":"guide/configuration/#7-version-your-configs","title":"7. Version Your Configs","text":"<p>Include version info for reproducibility:</p> <pre><code>config_version: \"1.0\"\ncreated: \"${now:%Y-%m-%d %H:%M:%S}\"\n</code></pre>"},{"location":"guide/configuration/#complete-example","title":"Complete Example","text":"<pre><code># config.py\nfrom dataclasses import dataclass, field\nfrom typing import Optional\nfrom expmate import Config\n\n@dataclass\nclass DataConfig(Config):\n    \"\"\"Data loading configuration\"\"\"\n    root: str = \"${DATA_ROOT}/imagenet\"\n    batch_size: int = 32\n    num_workers: int = 4\n    train_split: str = \"train\"\n    val_split: str = \"val\"\n\n@dataclass\nclass ModelConfig(Config):\n    \"\"\"Model architecture configuration\"\"\"\n    name: str = \"resnet50\"\n    pretrained: bool = True\n    num_classes: int = 1000\n    hidden_dim: int = 256\n    dropout: float = 0.1\n\n@dataclass\nclass OptimizerConfig(Config):\n    \"\"\"Optimizer configuration\"\"\"\n    name: str = \"adamw\"\n    lr: float = 0.001\n    weight_decay: float = 0.01\n    betas: tuple[float, float] = (0.9, 0.999)\n\n@dataclass\nclass TrainingConfig(Config):\n    \"\"\"Training configuration\"\"\"\n    epochs: int = 100\n    gradient_clip: float = 1.0\n    optimizer: OptimizerConfig = field(default_factory=OptimizerConfig)\n\n    def __post_init__(self):\n        if self.epochs &lt;= 0:\n            raise ValueError(\"epochs must be positive\")\n\n@dataclass\nclass ExperimentConfig(Config):\n    \"\"\"Main experiment configuration\"\"\"\n    run_id: str = \"exp_${now:%Y%m%d_%H%M%S}\"\n    seed: int = 42\n    device: str = \"cuda\"\n\n    data: DataConfig = field(default_factory=DataConfig)\n    model: ModelConfig = field(default_factory=ModelConfig)\n    training: TrainingConfig = field(default_factory=TrainingConfig)\n\n\n# train.py\nimport argparse\nfrom expmate import ExperimentLogger, set_seed\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Train model\")\n    parser.add_argument(\"config\", help=\"Path to config file\")\n    args, overrides = parser.parse_known_args()\n\n    # Load config with overrides\n    config = ExperimentConfig.from_file(args.config, overrides=overrides)\n\n    # Set seed for reproducibility\n    set_seed(config.seed)\n\n    # Initialize logger (automatically saves config)\n    logger = ExperimentLogger(run_dir=f\"runs/{config.run_id}\")\n    logger.info(f\"Starting experiment: {config.run_id}\")\n    logger.info(f\"Config: {config.to_yaml()}\")\n\n    # Train model\n    train(config, logger)\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>YAML config:</p> <pre><code># config.yaml\nseed: 42\ndevice: cuda\n\ndata:\n  root: /data/imagenet\n  batch_size: 64\n  num_workers: 8\n\nmodel:\n  name: resnet50\n  pretrained: true\n  hidden_dim: 256\n  dropout: 0.1\n\ntraining:\n  epochs: 100\n  gradient_clip: 1.0\n  optimizer:\n    name: adamw\n    lr: 0.001\n    weight_decay: 0.01\n</code></pre> <p>Usage:</p> <pre><code># Basic usage\npython train.py config.yaml\n\n# With overrides\npython train.py config.yaml \\\n    +model.hidden_dim=512 \\\n    +training.epochs=200 \\\n    +training.optimizer.lr=0.0001\n\n# With list overrides\npython train.py config.yaml \\\n    +data.gpu_ids:int 0 1 2 3 \\\n    +training.epochs=50\n</code></pre>"},{"location":"guide/configuration/#see-also","title":"See Also","text":"<ul> <li>Quick Start</li> <li>Basic Concepts</li> <li>API Reference: Config</li> <li>Examples: Linear Regression</li> </ul>"},{"location":"guide/distributed/","title":"Distributed Training","text":"<p>(Documentation in progress - will be added in next update)</p> <p>For now, see the DDP Example and examples.</p>"},{"location":"guide/logging/","title":"Experiment Logging","text":"<p>ExpMate provides a structured logging system for tracking experiments with colorful console output, hierarchical stage tracking, and rate-limited logging.</p>"},{"location":"guide/logging/#experimentlogger","title":"ExperimentLogger","text":"<p>The <code>ExperimentLogger</code> class is the main interface for logging.</p>"},{"location":"guide/logging/#basic-setup","title":"Basic Setup","text":"<pre><code>from expmate import ExperimentLogger\n\nlogger = ExperimentLogger(\n    run_dir='runs/exp1',\n    rank=0,              # Process rank (for distributed training)\n    run_id='exp1',       # Unique run identifier\n    log_level='INFO',    # Logging level\n    console_output=True  # Print to console\n)\n</code></pre>"},{"location":"guide/logging/#key-features","title":"Key Features","text":"<ul> <li>\ud83c\udfa8 Colorful Console Output: Automatic color-coding by log level (disabled in file logs)</li> <li>\u23f1\ufe0f Simple Timing: <code>timer()</code> context manager for wall-clock timing</li> <li>\ud83d\udcc2 Hierarchical Stages: <code>stage()</code> for tracking nested training phases</li> <li>\ud83d\udd07 Rate Limiting: <code>log_every()</code> to reduce console spam</li> <li>\ud83d\udcca Best Metrics: Automatic tracking of best model performance</li> <li>\ud83d\udd04 DDP Support: Rank-aware logging for distributed training</li> </ul>"},{"location":"guide/logging/#text-logging","title":"Text Logging","text":""},{"location":"guide/logging/#level-specific-methods-preferred","title":"Level-Specific Methods (Preferred)","text":"<pre><code>logger.info(\"Training started\")        # Green in console\nlogger.debug(\"Batch processing time\")  # Cyan in console\nlogger.warning(\"LR might be too high\") # Yellow in console\nlogger.error(\"NaN detected in loss\")   # Red in console\n</code></pre>"},{"location":"guide/logging/#color-output","title":"Color Output","text":"<p>Console output is automatically colorized when connected to a terminal: - DEBUG: Cyan - INFO: Green - WARNING: Yellow - ERROR: Red - Stage messages: Blue with bold stage names - Timer messages: Cyan</p> <p>Colors are automatically disabled when: - Output is redirected to a file - Output is piped to another command - Running in non-TTY environment</p> <p>File logs (<code>.log</code> files) and JSONL always use plain text without color codes.</p>"},{"location":"guide/logging/#generic-method","title":"Generic Method","text":"<pre><code>logger.log(\"Training started\", level=\"INFO\")\nlogger.log(\"Debug message\", level=\"DEBUG\")\n</code></pre>"},{"location":"guide/logging/#metrics-logging","title":"Metrics Logging","text":""},{"location":"guide/logging/#log-individual-metrics","title":"Log Individual Metrics","text":"<pre><code>logger.log_metric(\n    step=epoch,         # Training step/epoch\n    split='train',      # 'train', 'val', 'test', etc.\n    name='loss',        # Metric name\n    value=0.5          # Metric value\n)\n\nlogger.log_metric(step=epoch, split='train', name='accuracy', value=0.95)\nlogger.log_metric(step=epoch, split='val', name='loss', value=0.4)\n</code></pre>"},{"location":"guide/logging/#log-multiple-metrics","title":"Log Multiple Metrics","text":"<pre><code># Log several metrics at once\nfor name, value in metrics.items():\n    logger.log_metric(step=epoch, split='train', name=name, value=value)\n</code></pre>"},{"location":"guide/logging/#best-metrics-tracking","title":"Best Metrics Tracking","text":"<p>Track and save the best values for important metrics:</p> <pre><code># Metrics are automatically tracked when logging\nlogger.log_metric(step=epoch, split='val', name='loss', value=0.5, track_best=True, mode='min')\nlogger.log_metric(step=epoch, split='val', name='accuracy', value=0.95, track_best=True, mode='max')\n\n# Mode is auto-detected for common metric names\nlogger.log_metric(step=epoch, split='val', name='loss', value=0.5)  # Auto: mode='min'\nlogger.log_metric(step=epoch, split='val', name='accuracy', value=0.95)  # Auto: mode='max'\n</code></pre> <p>Access best metrics:</p> <pre><code>best_loss = logger.get_best_metric('loss', split='val')\nprint(f\"Best loss: {best_loss['value']} at step {best_loss['step']}\")\n</code></pre>"},{"location":"guide/logging/#timer-simple-profiling","title":"Timer (Simple Profiling)","text":"<p>Use <code>timer()</code> for simple wall-clock timing:</p> <pre><code># Basic timing\nwith logger.timer('data_loading'):\n    data = load_data()\n# Logs: \"Timer [data_loading]: 0.1234s\"\n\n# Get elapsed time\nwith logger.timer('training_step') as result:\n    loss = model(batch)\n    loss.backward()\nprint(f\"Step took {result['elapsed']:.4f}s\")\n\n# Silent timing (no logging)\nwith logger.timer('forward', log_result=False) as result:\n    output = model(input)\nelapsed = result['elapsed']\n</code></pre> <p>Note: For detailed GPU/CPU profiling, use <code>torch.profiler</code> directly. The <code>timer()</code> method provides simple wall-clock timing only.</p> <p>Global Control: Timing can be disabled globally: <pre><code>import expmate\nexpmate.timer = False  # Disable all timing\n</code></pre></p>"},{"location":"guide/logging/#hierarchical-stage-tracking","title":"Hierarchical Stage Tracking","text":"<p>Track training stages with nested hierarchies:</p> <pre><code># Simple stage\nwith logger.stage('training'):\n    train_model()\n# Logs: \"Stage [training] - START\" and \"Stage [training] - END (10.5s)\"\n\n# Stage with metadata\nwith logger.stage('epoch', epoch=5, lr=0.001):\n    train_epoch()\n# Logs: \"Stage [epoch] (epoch=5, lr=0.001) - START\"\n\n# Nested stages create hierarchies\nwith logger.stage('epoch', epoch=5):\n    with logger.stage('train'):\n        train_loss = train_epoch()\n    with logger.stage('validation'):\n        val_loss = validate()\n# Logs: \"Stage [epoch/train] - START\"\n#       \"Stage [epoch/validation] - START\"\n</code></pre> <p>Stage context is preserved in JSONL logs for analysis: <pre><code>{\"timestamp\": 1234567890, \"level\": \"INFO\", \"message\": \"Stage [epoch/train] - START\", \n \"stage\": \"epoch/train\", \"stage_event\": \"start\", \"epoch\": 5}\n</code></pre></p>"},{"location":"guide/logging/#rate-limited-logging","title":"Rate-Limited Logging","text":"<p>Reduce console spam in tight training loops:</p> <pre><code># Log every 100 iterations\nfor step in range(10000):\n    loss = train_step()\n    with logger.log_every(every=100):\n        logger.info(f\"Step {step}: loss={loss:.4f}\")\n# Logs to console every 100 steps, but ALL logs go to JSONL\n\n# Time-based rate limiting (every 5 seconds)\nfor batch in dataloader:\n    with logger.log_every(seconds=5.0):\n        logger.info(f\"Processing batch...\")\n\n# Multiple independent rate limiters\nfor step in range(1000):\n    with logger.log_every(every=10, key='loss'):\n        logger.info(f\"Loss: {loss:.4f}\")\n    with logger.log_every(every=100, key='detailed'):\n        logger.info(f\"Detailed metrics: {metrics}\")\n</code></pre> <p>Key Points: - Console output is suppressed when not logging - JSONL always captures all events - Auto-generated keys based on call location - Use custom <code>key</code> for multiple rate limiters</p>"},{"location":"guide/logging/#profiling-deprecated","title":"Profiling (Deprecated)","text":"<p>Note: The <code>profile()</code> method is deprecated. Use <code>timer()</code> instead for simple timing, or <code>torch.profiler</code> for detailed profiling.</p> <pre><code># OLD (deprecated but still works)\nwith logger.profile('data_loading'):\n    data = load_data()\n\n# NEW (recommended)\nwith logger.timer('data_loading'):\n    data = load_data()\n</code></pre>"},{"location":"guide/logging/#log-files","title":"Log Files","text":"<p>ExpMate creates several log files automatically:</p>"},{"location":"guide/logging/#human-readable-log-explog","title":"Human-Readable Log (<code>exp.log</code>)","text":"<pre><code>2025-01-23 14:30:22 - INFO - Starting experiment: exp_20250123_143022\n2025-01-23 14:30:23 - INFO - Epoch 0/10: loss=0.5234\n2025-01-23 14:30:24 - INFO - Epoch 1/10: loss=0.4567\n</code></pre>"},{"location":"guide/logging/#machine-readable-events-eventsjsonl","title":"Machine-Readable Events (<code>events.jsonl</code>)","text":"<pre><code>{\"timestamp\": \"2025-01-23T14:30:22\", \"level\": \"INFO\", \"message\": \"Starting experiment\"}\n{\"timestamp\": \"2025-01-23T14:30:23\", \"level\": \"INFO\", \"message\": \"Epoch 0/10: loss=0.5234\"}\n</code></pre>"},{"location":"guide/logging/#metrics-csv-metricscsv","title":"Metrics CSV (<code>metrics.csv</code>)","text":"<pre><code>step,split,name,value,wall_time\n0,train,loss,0.5234,1706017822\n1,train,loss,0.4567,1706017823\n0,val,loss,0.4123,1706017822\n</code></pre>"},{"location":"guide/logging/#best-metrics-bestjson","title":"Best Metrics (<code>best.json</code>)","text":"<pre><code>{\n  \"val_loss\": {\n    \"value\": 0.4123,\n    \"step\": 5,\n    \"mode\": \"min\"\n  },\n  \"val_accuracy\": {\n    \"value\": 0.9567,\n    \"step\": 8,\n    \"mode\": \"max\"\n  }\n}\n</code></pre>"},{"location":"guide/logging/#distributed-training","title":"Distributed Training","text":"<p>ExpMate supports rank-aware logging for distributed training:</p> <pre><code>from expmate.torch import mp\n\n# Setup DDP\nrank, local_rank, world_size = mp.setup_ddp()\n\n# Create logger with rank\nlogger = ExperimentLogger(\n    run_dir=run_dir,\n    rank=rank  # Each process has its own rank\n)\n\n# Rank 0 writes to main log files\n# Other ranks write to separate files (exp_rank1.log, etc.)\nif rank == 0:\n    logger.info(f\"Training on {world_size} GPUs\")\n\n# All ranks can log\nlogger.info(f\"Rank {rank}: Processing batch\")\n</code></pre>"},{"location":"guide/logging/#log-files-in-ddp","title":"Log Files in DDP","text":"<pre><code>runs/exp1/\n\u251c\u2500\u2500 exp.log                 # Rank 0 log\n\u251c\u2500\u2500 exp_rank1.log          # Rank 1 log\n\u251c\u2500\u2500 exp_rank2.log          # Rank 2 log\n\u251c\u2500\u2500 events.jsonl           # Rank 0 events\n\u251c\u2500\u2500 events_rank1.jsonl     # Rank 1 events\n\u251c\u2500\u2500 metrics.csv            # Rank 0 metrics (aggregated)\n\u2514\u2500\u2500 best.json              # Rank 0 best metrics\n</code></pre>"},{"location":"guide/logging/#complete-example","title":"Complete Example","text":"<pre><code>from expmate import ExperimentLogger, parse_config\n\n# Parse config\nconfig = parse_config()\n\n# Create logger\nlogger = ExperimentLogger(run_dir=f\"runs/{config.run_id}\")\nlogger.info(f\"Starting experiment: {config.run_id}\")\n\n# Training loop with hierarchical stages\nfor epoch in range(config.training.epochs):\n    with logger.stage('epoch', epoch=epoch):\n        # Training phase\n        with logger.stage('train'):\n            with logger.timer('train_epoch'):\n                train_loss, train_acc = train_epoch(model, train_loader)\n\n        logger.log_metric(step=epoch, split='train', name='loss', value=train_loss)\n        logger.log_metric(step=epoch, split='train', name='accuracy', value=train_acc)\n\n        # Validation phase\n        with logger.stage('validation'):\n            with logger.timer('val_epoch'):\n                val_loss, val_acc = validate(model, val_loader)\n\n        logger.log_metric(step=epoch, split='val', name='loss', value=val_loss)\n        logger.log_metric(step=epoch, split='val', name='accuracy', value=val_acc)\n\n        # Log epoch summary (with rate limiting)\n        with logger.log_every(every=1):  # Log every epoch\n            logger.info(\n                f\"Epoch {epoch}/{config.training.epochs}: \"\n                f\"train_loss={train_loss:.4f}, val_loss={val_loss:.4f}, \"\n                f\"val_acc={val_acc:.4f}\"\n            )\n\n# Print best results\nbest_loss = logger.get_best_metric('loss', split='val')\nbest_acc = logger.get_best_metric('accuracy', split='val')\nlogger.info(f\"Best validation loss: {best_loss['value']:.4f} at epoch {best_loss['step']}\")\nlogger.info(f\"Best validation accuracy: {best_acc['value']:.4f} at epoch {best_acc['step']}\")\nlogger.info(f\"Logs saved to: {logger.run_dir}\")\n</code></pre>"},{"location":"guide/logging/#best-practices","title":"Best Practices","text":""},{"location":"guide/logging/#1-use-descriptive-metric-names","title":"1. Use Descriptive Metric Names","text":"<pre><code># Good\nlogger.log_metric(step=epoch, split='val', name='top1_accuracy', value=acc1)\nlogger.log_metric(step=epoch, split='val', name='top5_accuracy', value=acc5)\n\n# Avoid\nlogger.log_metric(step=epoch, split='val', name='acc', value=acc)\n</code></pre>"},{"location":"guide/logging/#2-log-at-appropriate-frequency","title":"2. Log at Appropriate Frequency","text":"<pre><code># Log every epoch for validation metrics\nlogger.log_metric(step=epoch, split='val', name='loss', value=val_loss)\n\n# Use rate limiting for training metrics in tight loops\nfor step in range(10000):\n    loss = train_step()\n    with logger.log_every(every=100):\n        logger.info(f\"Step {step}: loss={loss:.4f}\")\n</code></pre>"},{"location":"guide/logging/#3-use-consistent-split-names","title":"3. Use Consistent Split Names","text":"<pre><code># Standard splits\nlogger.log_metric(step=epoch, split='train', name='loss', value=loss)\nlogger.log_metric(step=epoch, split='val', name='loss', value=loss)\nlogger.log_metric(step=epoch, split='test', name='loss', value=loss)\n</code></pre>"},{"location":"guide/logging/#4-track-important-metrics","title":"4. Track Important Metrics","text":"<pre><code># Metrics are automatically tracked with auto-detection\nlogger.log_metric(step=epoch, split='val', name='loss', value=val_loss)  # mode='min'\nlogger.log_metric(step=epoch, split='val', name='f1_score', value=f1)    # mode='max'\nlogger.log_metric(step=epoch, split='val', name='perplexity', value=ppl) # mode='min'\n</code></pre>"},{"location":"guide/logging/#5-use-timer-for-critical-sections","title":"5. Use Timer for Critical Sections","text":"<pre><code># Time important operations\nwith logger.timer('data_loading'):\n    batch = next(dataloader)\n\nwith logger.timer('forward_backward'):\n    loss = model(batch)\n    loss.backward()\n\nwith logger.timer('optimizer_step'):\n    optimizer.step()\n</code></pre>"},{"location":"guide/logging/#6-use-hierarchical-stages","title":"6. Use Hierarchical Stages","text":"<pre><code># Organize training with stages\nwith logger.stage('epoch', epoch=epoch):\n    with logger.stage('train'):\n        train_loss = train_epoch()\n    with logger.stage('validation'):\n        val_loss = validate()\n    with logger.stage('checkpoint'):\n        save_checkpoint(model)\n</code></pre>"},{"location":"guide/logging/#advanced-features","title":"Advanced Features","text":""},{"location":"guide/logging/#custom-log-formatting","title":"Custom Log Formatting","text":"<pre><code>import logging\n\n# Create custom handler with formatter\nhandler = logging.FileHandler('custom.log')\nhandler.setFormatter(\n    logging.Formatter('%(asctime)s | %(levelname)s | %(message)s')\n)\nlogger.log_handler = handler\n</code></pre>"},{"location":"guide/logging/#reading-logs-programmatically","title":"Reading Logs Programmatically","text":"<pre><code>import json\nimport pandas as pd\n\n# Read metrics CSV\nmetrics_df = pd.read_csv('runs/exp1/metrics.csv')\ntrain_loss = metrics_df[\n    (metrics_df['split'] == 'train') &amp; \n    (metrics_df['name'] == 'loss')\n]\n\n# Read events JSONL\nevents = []\nwith open('runs/exp1/events.jsonl') as f:\n    for line in f:\n        events.append(json.loads(line))\n\n# Read best metrics\nwith open('runs/exp1/best.json') as f:\n    best_metrics = json.load(f)\n</code></pre>"},{"location":"guide/logging/#see-also","title":"See Also","text":"<ul> <li>Configuration Management</li> <li>Checkpoint Management</li> <li>CLI Tools</li> <li>API Reference: Logger</li> </ul>"},{"location":"guide/tracking/","title":"Experiment Tracking","text":"<p>(Documentation in progress - will be added in next update)</p> <p>For now, see the API Reference.</p>"}]}